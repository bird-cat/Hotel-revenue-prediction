{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error, make_scorer, confusion_matrix, log_loss\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression, SGDRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, RandomForestRegressor, RandomForestClassifier, StackingRegressor, StackingClassifier\n",
    "from xgboost.sklearn import XGBRegressor, XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.kernel_approximation import Nystroem\n",
    "from joblib import dump, load\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "raw_train_df = pd.read_csv(\"train.csv\")\n",
    "raw_train_label_df = pd.read_csv('train_label.csv')\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into validation set and training set\n",
    "dates = raw_train_df.arrival_date.unique()\n",
    "train_indices, val_indices = train_test_split(dates, test_size=0.3, random_state=216)\n",
    "train_df = raw_train_df.set_index('arrival_date').loc[train_indices, :]\n",
    "val_df = raw_train_df.set_index('arrival_date').loc[val_indices, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=False, inplace=True)\n",
    "val_df.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer\n",
    "\n",
    "# Transforming the categoric columns\n",
    "cat_si_step = ('si', SimpleImputer(strategy='constant', fill_value='MISSING'))\n",
    "cat_ohe_step = ('ohe', OneHotEncoder(sparse=True, handle_unknown='ignore'))\n",
    "cat_steps = [cat_si_step, cat_ohe_step]\n",
    "cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "# Transforming the numeric columns\n",
    "num_si_step = ('si', SimpleImputer(strategy='median'))\n",
    "num_ss_step = ('ss', StandardScaler())\n",
    "num_steps = [num_si_step, num_ss_step]\n",
    "num_pipe = Pipeline(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1: Predict ADR by regression\n",
    "# Determine features\n",
    "adr_numericCols = ['lead_time', 'arrival_date_year', 'stays', 'stays_in_weekend_nights', 'stays_in_week_nights',\\\n",
    "               'adults', 'children', 'babies', 'persons', 'previous_cancellations',\\\n",
    "               'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list',\\\n",
    "               'required_car_parking_spaces', 'total_of_special_requests'\n",
    "              ]\n",
    "adr_categoricCols = ['hotel', 'arrival_date_month',\\\n",
    "                'arrival_date_week_number', 'arrival_date_day_of_month', 'meal',\\\n",
    "                'country', 'market_segment', 'distribution_channel',\\\n",
    "                'is_repeated_guest', 'reserved_room_type', 'assigned_room_type',\\\n",
    "                'deposit_type', 'customer_type', 'agent', 'company'\n",
    "               ]\n",
    "\n",
    "adr_featureCols = adr_numericCols + adr_categoricCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adr_train = train_df[adr_featureCols]\n",
    "adr_val = val_df[adr_featureCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining both categorical and numerical column transformations\n",
    "adr_ct = ColumnTransformer(transformers=[('cat', cat_pipe, adr_categoricCols), ('num', num_pipe, adr_numericCols)])\n",
    "adr_train_transformed = adr_ct.fit_transform(adr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of features =  857\n"
     ]
    }
   ],
   "source": [
    "# Retrieving the feature names\n",
    "cat_transformers = [('cat', cat_pipe, adr_categoricCols)]\n",
    "cat_ct = ColumnTransformer(transformers=cat_transformers)\n",
    "train_cat_transformed = cat_ct.fit_transform(train_df)\n",
    "\n",
    "num_transformers = [('num', num_pipe, adr_numericCols)]\n",
    "num_ct = ColumnTransformer(transformers=num_transformers)\n",
    "train_num_transformed = num_ct.fit_transform(train_df)\n",
    "\n",
    "cat_pl = cat_ct.named_transformers_['cat']\n",
    "ohe = cat_pl.named_steps['ohe']\n",
    "transformed_feature_names = list(ohe.get_feature_names()) + adr_numericCols\n",
    "print(\"Total number of features = \", len(transformed_feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "adr_sel = VarianceThreshold(threshold=(.9 * (1 - .9)))\n",
    "adr_svd = TruncatedSVD(n_components=40) # best 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression\n",
    "ridge = Ridge(alpha=1.0/(2*100), max_iter=10000) # alpha = 1 / 2C\n",
    "ridge_pipe = Pipeline([('adr_transform', adr_ct), ('ridge', ridge)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5396044840978833\n",
      "0.6039522121224565\n",
      "1224.477257650087\n",
      "794.8081163227173\n"
     ]
    }
   ],
   "source": [
    "# Execute ridge pipeline\n",
    "ridge_pipe.fit(adr_train, train_df['adr'])\n",
    "\n",
    "# Correlation coefficients\n",
    "print(ridge_pipe.score(adr_train, train_df['adr']))\n",
    "print(ridge_pipe.score(adr_val, val_df['adr']))\n",
    "\n",
    "# Mean squared errors\n",
    "print(mean_squared_error(ridge_pipe.predict(adr_train), train_df['adr']))\n",
    "print(mean_squared_error(ridge_pipe.predict(adr_val), val_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] ridge__alpha=0.01 ...............................................\n",
      "[CV] ................... ridge__alpha=0.01, score=0.654, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.7s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.01 ...............................................\n",
      "[CV] ................... ridge__alpha=0.01, score=0.381, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    3.5s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.01 ...............................................\n",
      "[CV] ................... ridge__alpha=0.01, score=0.665, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    5.1s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.01 ...............................................\n",
      "[CV] ................... ridge__alpha=0.01, score=0.664, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    6.9s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.01 ...............................................\n",
      "[CV] ................... ridge__alpha=0.01, score=0.662, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    8.6s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.006666666666666667 ...............................\n",
      "[CV] ... ridge__alpha=0.006666666666666667, score=0.654, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   10.3s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.006666666666666667 ...............................\n",
      "[CV] ... ridge__alpha=0.006666666666666667, score=0.381, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:   12.0s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.006666666666666667 ...............................\n",
      "[CV] ... ridge__alpha=0.006666666666666667, score=0.665, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:   13.7s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.006666666666666667 ...............................\n",
      "[CV] ... ridge__alpha=0.006666666666666667, score=0.664, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:   15.5s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.006666666666666667 ...............................\n",
      "[CV] ... ridge__alpha=0.006666666666666667, score=0.662, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:   17.2s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.005 ..............................................\n",
      "[CV] .................. ridge__alpha=0.005, score=0.654, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:   18.8s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.005 ..............................................\n",
      "[CV] .................. ridge__alpha=0.005, score=0.381, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:   20.5s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.005 ..............................................\n",
      "[CV] .................. ridge__alpha=0.005, score=0.665, total=   1.6s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:   22.2s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.005 ..............................................\n",
      "[CV] .................. ridge__alpha=0.005, score=0.664, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:   24.0s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.005 ..............................................\n",
      "[CV] .................. ridge__alpha=0.005, score=0.662, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:   25.7s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.004 ..............................................\n",
      "[CV] .................. ridge__alpha=0.004, score=0.654, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:   27.4s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.004 ..............................................\n",
      "[CV] .................. ridge__alpha=0.004, score=0.381, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:   29.0s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.004 ..............................................\n",
      "[CV] .................. ridge__alpha=0.004, score=0.665, total=   1.6s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:   30.7s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.004 ..............................................\n",
      "[CV] .................. ridge__alpha=0.004, score=0.664, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:   32.5s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.004 ..............................................\n",
      "[CV] .................. ridge__alpha=0.004, score=0.662, total=   1.8s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:   34.3s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.0033333333333333335 ..............................\n",
      "[CV] .. ridge__alpha=0.0033333333333333335, score=0.654, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:   36.0s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.0033333333333333335 ..............................\n",
      "[CV] .. ridge__alpha=0.0033333333333333335, score=0.381, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:   37.6s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.0033333333333333335 ..............................\n",
      "[CV] .. ridge__alpha=0.0033333333333333335, score=0.665, total=   1.6s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:   39.2s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.0033333333333333335 ..............................\n",
      "[CV] .. ridge__alpha=0.0033333333333333335, score=0.664, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:   40.9s remaining:    0.0s\n",
      "[CV] ridge__alpha=0.0033333333333333335 ..............................\n",
      "[CV] .. ridge__alpha=0.0033333333333333335, score=0.662, total=   1.7s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   42.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:   42.6s finished\n",
      "{'ridge__alpha': 0.006666666666666667}\n",
      "0.605020104196267\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       1.613810      0.031798         0.106241        0.001378   \n",
      "1       1.603106      0.049526         0.105780        0.001058   \n",
      "2       1.588346      0.060087         0.105589        0.001030   \n",
      "3       1.614161      0.062565         0.106409        0.000965   \n",
      "4       1.546598      0.041126         0.106363        0.001545   \n",
      "\n",
      "  param_ridge__alpha                                   params  \\\n",
      "0               0.01                   {'ridge__alpha': 0.01}   \n",
      "1         0.00666667   {'ridge__alpha': 0.006666666666666667}   \n",
      "2              0.005                  {'ridge__alpha': 0.005}   \n",
      "3              0.004                  {'ridge__alpha': 0.004}   \n",
      "4         0.00333333  {'ridge__alpha': 0.0033333333333333335}   \n",
      "\n",
      "   split0_test_score  split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0           0.653850           0.380734           0.664698           0.663973   \n",
      "1           0.654047           0.380794           0.664696           0.663811   \n",
      "2           0.654003           0.380789           0.664636           0.663808   \n",
      "3           0.653839           0.380791           0.664643           0.663889   \n",
      "4           0.653849           0.380741           0.664586           0.663969   \n",
      "\n",
      "   split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0           0.661789         0.605009        0.112204                2  \n",
      "1           0.661753         0.605020        0.112176                1  \n",
      "2           0.661755         0.604998        0.112168                3  \n",
      "3           0.661753         0.604983        0.112161                4  \n",
      "4           0.661754         0.604980        0.112185                5  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'ridge__alpha': [1.0/(2*50), 1.0/(2*75), 1.0/(2*100), 1.0/(2*125), 1.0/(2*150)]\n",
    "}\n",
    "\n",
    "raw_adr_train = raw_train_df[adr_featureCols]\n",
    "gs = GridSearchCV(ridge_pipe, param_grid, cv=kf, verbose=100, n_jobs=1)\n",
    "gs.fit(raw_adr_train, raw_train_df['adr'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR\n",
    "rbf_feature = Nystroem(gamma=10**(-1.5), n_components=600, random_state=1126)\n",
    "sgdreg = SGDRegressor(loss='epsilon_insensitive', alpha=10**(-11), max_iter=10000)\n",
    "\n",
    "sgd_pipe = Pipeline([('adr_transform', adr_ct), ('rbf', rbf_feature), ('sgdreg', sgdreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38272605380374847\n",
      "0.45698674243912296\n",
      "1591.1612936139725\n",
      "1179.9846833209801\n"
     ]
    }
   ],
   "source": [
    "# Execute svr pipeline\n",
    "sgd_pipe.fit(adr_train, train_df['adr'])\n",
    "\n",
    "# Correlation coefficients\n",
    "print(sgd_pipe.score(adr_train, train_df['adr']))\n",
    "print(sgd_pipe.score(adr_val, val_df['adr']))\n",
    "\n",
    "# Mean squared errors\n",
    "print(mean_squared_error(sgd_pipe.predict(adr_train), train_df['adr']))\n",
    "print(mean_squared_error(sgd_pipe.predict(adr_val), val_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting regressor\n",
    "gbreg = GradientBoostingRegressor(loss='huber',\n",
    "                                  alpha=0.9,\n",
    "                                  learning_rate=0.1,\n",
    "                                  validation_fraction=0.2,\n",
    "                                  n_iter_no_change=5,\n",
    "                                  tol=0.005,\n",
    "                                  max_depth=6,\n",
    "                                  subsample=0.8,\n",
    "                                  n_estimators=10000,\n",
    "                                  random_state=1126,\n",
    "                                  verbose=True)\n",
    "gbreg_pipe = Pipeline([('adr_transform', adr_ct), ('gbreg', gbreg)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing adr_transform, total=   0.7s\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         791.0031          71.2462           89.58m\n",
      "         2         710.3068          79.9412           85.63m\n",
      "         3         640.6585          67.5975           83.62m\n",
      "         4         582.5683          55.5360           82.94m\n",
      "         5         537.3059          46.3521           83.74m\n",
      "         6         498.8246          37.9991           83.69m\n",
      "         7         459.6104          32.2106           83.31m\n",
      "         8         429.8022          30.7081           83.80m\n",
      "         9         408.0195          24.1427           83.32m\n",
      "        10         386.4751          20.0305           83.78m\n",
      "        20         257.6708           7.2525           83.78m\n",
      "        30         200.1122           5.4780           83.26m\n",
      "        40         172.5247           2.4070           77.94m\n",
      "        50         157.1923           1.3468           74.77m\n",
      "        60         143.6420           1.0636           72.03m\n",
      "        70         131.8138           0.5666           70.53m\n",
      "        80         123.2153           0.7415           68.68m\n",
      "        90         116.8145           0.2775           66.75m\n",
      "       100         114.9614           0.3423           65.17m\n",
      "       200          92.2354           0.0300           54.09m\n",
      "       300          81.4316           0.0346           49.48m\n",
      "       400          74.4458          -0.0062           46.28m\n",
      "       500          69.2573           0.0096           43.79m\n",
      "       600          64.3287          -0.0041           42.19m\n",
      "       700          62.5680          -0.0062           40.78m\n",
      "       800          58.8476           0.0545           39.67m\n",
      "       900          56.2401          -0.0174           38.69m\n",
      "[Pipeline] ............. (step 2 of 2) Processing gbreg, total= 4.1min\n",
      "0.8135135990738611\n",
      "0.6211356063358019\n",
      "485.2510841462102\n",
      "798.1646104899921\n"
     ]
    }
   ],
   "source": [
    "# Execute gradient boosting pipeline\n",
    "gbreg_pipe.fit(adr_train, train_df['adr'])\n",
    "\n",
    "# Correlation coefficients\n",
    "print(gbreg_pipe.score(adr_train, train_df['adr']))\n",
    "print(gbreg_pipe.score(adr_val, val_df['adr']))\n",
    "\n",
    "# Mean squared errors\n",
    "print(mean_squared_error(gbreg_pipe.predict(adr_train), train_df['adr']))\n",
    "print(mean_squared_error(gbreg_pipe.predict(adr_val), val_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] gbreg__learning_rate=0.1, gbreg__subsample=0.8 ..................\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         942.3188          59.9080           43.32m\n",
      "         2         877.3441          57.1465           41.59m\n",
      "         3         834.1607          47.5389           41.10m\n",
      "         4         789.4696          40.9935           41.09m\n",
      "         5         743.2766          49.8062           41.21m\n",
      "         6         707.4602          38.3583           40.98m\n",
      "         7         665.1406          36.4716           41.23m\n",
      "         8         633.7590          36.0146           41.00m\n",
      "         9         605.1403          25.8282           41.02m\n",
      "        10         580.1455          26.9657           41.60m\n",
      "        20         433.2342           8.7163           41.54m\n",
      "        30         367.3260           4.1714           39.40m\n",
      "        40         321.4118           3.8667           38.56m\n",
      "        50         289.1799           2.3173           37.88m\n",
      "        60         264.0311           1.6835           37.55m\n",
      "        70         243.8486           1.8013           37.45m\n",
      "        80         229.2097           1.0965           37.12m\n",
      "        90         217.0922           0.5863           36.89m\n",
      "       100         208.8388           0.5796           36.45m\n",
      "       200         162.8773           0.1571           33.48m\n",
      "       300         144.0958           0.0388           31.84m\n",
      "       400         132.5170          -0.0021           30.68m\n",
      "       500         123.7455           0.0039           29.83m\n",
      "       600         119.1989           0.0764           29.12m\n",
      "       700         113.8938          -0.0159           28.48m\n",
      "       800         110.3061           0.0180           27.96m\n",
      "[CV]  gbreg__learning_rate=0.1, gbreg__subsample=0.8, score=0.848, total= 2.5min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  2.5min remaining:    0.0s\n",
      "[CV] gbreg__learning_rate=0.1, gbreg__subsample=0.8 ..................\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         936.6132          62.3694           42.51m\n",
      "         2         882.2976          57.6823           41.09m\n",
      "         3         830.4754          47.2511           40.49m\n",
      "         4         790.2156          41.1275           40.39m\n",
      "         5         747.8750          48.2689           40.60m\n",
      "         6         705.0327          39.7042           40.49m\n",
      "         7         666.9658          35.7230           40.70m\n",
      "         8         629.1899          37.2070           40.36m\n",
      "         9         602.6226          29.1020           41.12m\n",
      "        10         575.0732          22.8750           41.13m\n",
      "        20         433.2405           8.7126           41.02m\n",
      "        30         355.8240           4.8025           40.13m\n",
      "        40         317.1969           2.8206           39.02m\n",
      "        50         287.8158           1.4294           38.15m\n",
      "        60         259.4319           1.7935           37.89m\n",
      "        70         240.4235           1.3186           37.62m\n",
      "        80         225.3408           1.9002           37.28m\n",
      "        90         216.1184           0.9113           36.92m\n",
      "       100         207.9600           0.5609           36.35m\n",
      "       200         161.3105           0.1151           33.43m\n",
      "       300         142.9876           0.0240           31.77m\n",
      "       400         130.4143           0.0757           30.84m\n",
      "       500         124.3238           0.0071           30.03m\n",
      "       600         116.6844           0.0469           29.36m\n",
      "       700         112.9462           0.0113           28.81m\n",
      "       800         109.6628           0.0260           28.24m\n",
      "       900         105.6014          -0.0045           27.71m\n",
      "      1000         102.4009          -0.0006           27.28m\n",
      "[CV]  gbreg__learning_rate=0.1, gbreg__subsample=0.8, score=0.498, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.1min remaining:    0.0s\n",
      "[CV] gbreg__learning_rate=0.1, gbreg__subsample=0.8 ..................\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         939.0297          60.1884           43.04m\n",
      "         2         880.9566          56.7579           41.45m\n",
      "         3         829.2546          47.7009           41.09m\n",
      "         4         789.6387          39.1392           40.69m\n",
      "         5         746.0383          47.6492           40.94m\n",
      "         6         694.5787          47.3071           40.74m\n",
      "         7         655.5796          36.9495           40.97m\n",
      "         8         623.8364          34.7123           40.82m\n",
      "         9         597.3358          26.7368           41.17m\n",
      "        10         564.4897          27.7932           41.61m\n",
      "        20         418.4209           9.1312           42.25m\n",
      "        30         357.0260           5.2838           40.74m\n",
      "        40         317.2467           2.7405           39.32m\n",
      "        50         282.8806           5.7762           38.72m\n",
      "        60         258.4871           1.9852           38.40m\n",
      "        70         239.8755           1.5466           37.94m\n",
      "        80         222.3550           1.2416           37.73m\n",
      "        90         213.3780           0.8303           37.28m\n",
      "       100         204.0544           0.7968           37.02m\n",
      "       200         160.4460           0.1707           33.94m\n",
      "       300         142.1775           0.0518           32.32m\n",
      "       400         130.6669          -0.0069           31.21m\n",
      "       500         122.8608          -0.0036           30.42m\n",
      "       600         116.8619          -0.0121           29.65m\n",
      "       700         112.4172           0.0007           29.08m\n",
      "       800         107.8439           0.0187           28.58m\n",
      "       900         104.9035           0.0598           28.07m\n",
      "      1000         101.5291           0.0797           27.63m\n",
      "[CV]  gbreg__learning_rate=0.1, gbreg__subsample=0.8, score=0.867, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  9.5min remaining:    0.0s\n",
      "[CV] gbreg__learning_rate=0.1, gbreg__subsample=0.8 ..................\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         931.9736          62.3286           42.82m\n",
      "         2         885.5613          55.5189           41.36m\n",
      "         3         837.1414          47.8222           40.85m\n",
      "         4         786.1486          54.4285           41.10m\n",
      "         5         749.7287          34.6506           40.47m\n",
      "         6         707.4184          41.0771           40.73m\n",
      "         7         663.5991          41.8691           40.51m\n",
      "         8         631.3669          30.5534           40.53m\n",
      "         9         605.1759          26.1098           40.23m\n",
      "        10         582.4790          26.7966           40.75m\n",
      "        20         436.9974           8.9165           41.35m\n",
      "        30         367.7059           5.8727           39.59m\n",
      "        40         326.5750           3.4040           38.55m\n",
      "        50         292.0710           4.2168           37.92m\n",
      "        60         265.5126           1.4307           37.54m\n",
      "        70         245.1148           1.5849           37.09m\n",
      "        80         234.5770           0.9177           36.57m\n",
      "        90         221.4439           1.3987           36.25m\n",
      "       100         212.1709           2.1534           35.85m\n",
      "       200         160.0124           0.2622           33.47m\n",
      "       300         145.2881           0.0558           31.90m\n",
      "       400         133.9052           0.0073           30.85m\n",
      "       500         122.3427           0.0143           30.00m\n",
      "       600         119.2894           0.0024           29.31m\n",
      "       700         114.6591           0.0656           28.67m\n",
      "       800         110.9303           0.0067           28.18m\n",
      "       900         107.9469          -0.0210           27.65m\n",
      "[CV]  gbreg__learning_rate=0.1, gbreg__subsample=0.8, score=0.861, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 12.3min remaining:    0.0s\n",
      "[CV] gbreg__learning_rate=0.1, gbreg__subsample=0.8 ..................\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         928.9548          60.2178           42.49m\n",
      "         2         882.7147          55.8429           41.36m\n",
      "         3         830.4675          48.0244           40.85m\n",
      "         4         798.1233          38.5920           40.81m\n",
      "         5         750.2400          47.3734           40.96m\n",
      "         6         700.5069          46.4044           40.58m\n",
      "         7         662.7893          35.5049           40.85m\n",
      "         8         637.5482          27.7799           40.88m\n",
      "         9         610.8203          22.8240           40.62m\n",
      "        10         582.2576          27.1666           41.09m\n",
      "        20         436.8173          10.5485           40.87m\n",
      "        30         362.7335           6.0833           40.87m\n",
      "        40         321.6664           2.5576           39.65m\n",
      "        50         286.9727           1.8990           38.72m\n",
      "        60         268.2783           1.6145           38.13m\n",
      "        70         245.4985           1.9206           37.52m\n",
      "        80         232.4456           1.4951           37.23m\n",
      "        90         222.7819           1.4485           36.63m\n",
      "       100         215.1812           0.3656           36.13m\n",
      "       200         162.3079           0.0868           33.48m\n",
      "       300         146.4264           0.2663           31.77m\n",
      "       400         134.6611           0.0129           30.77m\n",
      "       500         126.2207           0.0409           30.08m\n",
      "       600         120.6076           0.0100           29.29m\n",
      "       700         116.1356          -0.0218           28.71m\n",
      "       800         108.7542           0.1113           28.22m\n",
      "       900         107.9676           0.0044           27.73m\n",
      "      1000         105.0257          -0.0165           27.28m\n",
      "[CV]  gbreg__learning_rate=0.1, gbreg__subsample=0.8, score=0.866, total= 3.1min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 15.4min finished\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1         941.6702          60.5673           54.30m\n",
      "         2         891.7023          56.6859           52.00m\n",
      "         3         836.7449          47.5516           51.22m\n",
      "         4         784.4729          54.4649           51.27m\n",
      "         5         748.3002          36.0268           51.22m\n",
      "         6         705.8893          42.1133           51.40m\n",
      "         7         666.6011          42.3181           50.92m\n",
      "         8         628.6864          36.3701           50.62m\n",
      "         9         596.8251          26.3874           50.70m\n",
      "        10         575.4854          25.5244           50.43m\n",
      "        20         432.1173           8.4180           51.13m\n",
      "        30         351.8820           5.6872           50.98m\n",
      "        40         313.2926           3.1962           49.38m\n",
      "        50         280.2658           2.6754           48.64m\n",
      "        60         258.1137           1.7654           48.12m\n",
      "        70         237.2605           1.0231           47.64m\n",
      "        80         222.4034           1.6536           47.14m\n",
      "        90         215.2935           1.4290           46.44m\n",
      "       100         206.8051           1.0899           45.83m\n",
      "       200         164.0316           0.1410           41.91m\n",
      "       300         146.7537           0.1235           39.92m\n",
      "       400         135.5248           0.2104           38.42m\n",
      "       500         128.7662           0.0534           37.37m\n",
      "       600         121.2960           0.0261           36.73m\n",
      "       700         117.2865           0.0007           35.90m\n",
      "       800         114.2156           0.0054           35.22m\n",
      "       900         110.4152          -0.0058           34.64m\n",
      "      1000         107.7356           0.0405           34.09m\n",
      "{'gbreg__learning_rate': 0.1, 'gbreg__subsample': 0.8}\n",
      "0.7881068598095102\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0     184.549252     23.718259         0.582155        0.069276   \n",
      "\n",
      "  param_gbreg__learning_rate param_gbreg__subsample  \\\n",
      "0                        0.1                    0.8   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'gbreg__learning_rate': 0.1, 'gbreg__subsampl...           0.847959   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0           0.498061           0.867495           0.860648           0.866372   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0         0.788107        0.145189                1  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'gbreg__learning_rate': [0.1],\n",
    "    'gbreg__subsample': [0.8]\n",
    "}\n",
    "\n",
    "raw_adr_train = raw_train_df[adr_featureCols]\n",
    "gs = GridSearchCV(gbreg_pipe, param_grid, cv=kf, verbose=100, n_jobs=1)\n",
    "gs.fit(raw_adr_train, raw_train_df['adr'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost regressor\n",
    "xgbreg = XGBRegressor(booster='gbtree',\n",
    "                      objective='reg:pseudohubererror',\n",
    "                      learning_rate=0.04,\n",
    "                      n_estimators=10000,\n",
    "                      subsample=0.8,\n",
    "                      colsample_bytree=0.6, # best\n",
    "                      max_depth=6,\n",
    "                      gamma=5,\n",
    "                      #reg_lambda=30.0,\n",
    "                      random_state=1126)\n",
    "xgbreg_pipe = Pipeline([('adr_transform', adr_ct), ('xgbreg', xgbreg)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8360627481665961\n",
      "0.705959108168488\n",
      "426.57656959993005\n",
      "619.4644781131004\n"
     ]
    }
   ],
   "source": [
    "# Execute XGBoost pipeline\n",
    "xgbreg_pipe.fit(adr_train, train_df['adr'])\n",
    "\n",
    "# Correlation coefficients\n",
    "print(xgbreg_pipe.score(adr_train, train_df['adr']))\n",
    "print(xgbreg_pipe.score(adr_val, val_df['adr']))\n",
    "\n",
    "# Mean squared errors\n",
    "print(mean_squared_error(xgbreg_pipe.predict(adr_train), train_df['adr']))\n",
    "print(mean_squared_error(xgbreg_pipe.predict(adr_val), val_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.6, score=0.858, total= 3.0min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.6, score=0.498, total= 3.0min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.0min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.6, score=0.867, total= 3.0min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  9.0min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.6, score=0.860, total= 3.0min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 11.9min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.6, score=0.864, total= 3.0min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 14.9min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.8, score=0.860, total= 2.7min\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed: 17.6min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.8, score=0.500, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed: 20.2min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.8, score=0.867, total= 2.7min\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed: 22.9min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.8, score=0.857, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed: 25.5min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.01, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.01, xgbreg__subsample=0.8, score=0.865, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed: 28.2min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.6, score=0.873, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed: 31.1min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.6, score=0.504, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed: 34.0min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.6, score=0.870, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed: 37.0min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.6, score=0.860, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 39.9min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.6 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.6, score=0.866, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 42.9min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.8, score=0.869, total= 2.9min\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 45.8min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.8, score=0.508, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 48.4min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.8, score=0.864, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 51.1min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.8, score=0.828, total= 2.6min\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 53.7min remaining:    0.0s\n",
      "[CV] xgbreg__learning_rate=0.04, xgbreg__subsample=0.8 ...............\n",
      "[CV]  xgbreg__learning_rate=0.04, xgbreg__subsample=0.8, score=0.864, total= 2.7min\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 56.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 56.4min finished\n",
      "{'xgbreg__learning_rate': 0.04, 'xgbreg__subsample': 0.6}\n",
      "0.7944602628970008\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0     178.205347      0.865207         0.953421        0.012478   \n",
      "1     157.889242      0.636666         0.931255        0.014127   \n",
      "2     175.551778      0.180363         0.933182        0.028823   \n",
      "3     161.561737      6.840262         0.926447        0.021061   \n",
      "\n",
      "  param_xgbreg__learning_rate param_xgbreg__subsample  \\\n",
      "0                        0.01                     0.6   \n",
      "1                        0.01                     0.8   \n",
      "2                        0.04                     0.6   \n",
      "3                        0.04                     0.8   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'xgbreg__learning_rate': 0.01, 'xgbreg__subsa...           0.858326   \n",
      "1  {'xgbreg__learning_rate': 0.01, 'xgbreg__subsa...           0.859958   \n",
      "2  {'xgbreg__learning_rate': 0.04, 'xgbreg__subsa...           0.872557   \n",
      "3  {'xgbreg__learning_rate': 0.04, 'xgbreg__subsa...           0.869231   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0           0.498195           0.867273           0.860404           0.864135   \n",
      "1           0.500244           0.867140           0.857229           0.865272   \n",
      "2           0.503868           0.870315           0.859970           0.865591   \n",
      "3           0.508072           0.863665           0.828336           0.863636   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0         0.789667        0.145768                3  \n",
      "1         0.789968        0.144906                2  \n",
      "2         0.794460        0.145360                1  \n",
      "3         0.786588        0.140015                4  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'xgbreg__learning_rate': [0.01, 0.04],\n",
    "    'xgbreg__subsample': [0.6, 0.8]\n",
    "}\n",
    "\n",
    "raw_adr_train = raw_train_df[adr_featureCols]\n",
    "gs = GridSearchCV(xgbreg_pipe, param_grid, cv=kf, verbose=100, n_jobs=1)\n",
    "gs.fit(raw_adr_train, raw_train_df['adr'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.49277672e-02 9.48060583e-03 1.77365742e-04 3.03066336e-02\n",
      " 2.31166696e-03 4.66068229e-03 4.99157654e-03 2.07430180e-02\n",
      " 5.98067651e-03 3.26931179e-02 3.41507723e-03 3.04109184e-03\n",
      " 2.53399927e-03 8.70812312e-03 5.38866036e-04 2.12808518e-04\n",
      " 2.75918952e-04 1.14784183e-04 3.42918502e-04 3.64371925e-04\n",
      " 3.11867916e-04 1.48078252e-04 4.95678571e-04 4.09991451e-04\n",
      " 4.10673121e-04 2.97940656e-04 5.09187989e-02 2.49428645e-04\n",
      " 1.26498835e-02 2.54907878e-04 8.68205243e-05 7.80449947e-04\n",
      " 4.36787610e-04 3.39835795e-04 3.38189449e-04 2.18848675e-03\n",
      " 3.98098229e-04 7.05795246e-04 1.65221898e-03 3.64407024e-04\n",
      " 1.24695012e-03 1.77179917e-03 3.07910284e-03 5.02198795e-03\n",
      " 5.55258384e-03 5.38135972e-03 1.58032924e-02 9.43005737e-03\n",
      " 4.60958667e-03 1.44193089e-03 1.91030733e-03 7.49319501e-04\n",
      " 7.96393212e-03 1.32569799e-03 1.39154051e-03 7.52143445e-04\n",
      " 6.13126147e-04 1.10531051e-03 6.12902280e-04 2.28994503e-03\n",
      " 9.31993884e-04 1.74645940e-03 1.47510681e-03 3.53727693e-04\n",
      " 9.96627379e-04 6.23700151e-04 6.54069986e-03 4.47879283e-04\n",
      " 5.26018965e-04 2.47199088e-04 3.94204311e-04 4.25100356e-04\n",
      " 1.02396430e-02 1.42831705e-03 3.25512345e-04 2.61182693e-04\n",
      " 3.19191458e-04 3.81009129e-04 4.54674620e-04 3.62060324e-04\n",
      " 3.32143420e-04 3.37276084e-04 3.13084893e-04 1.97170259e-04\n",
      " 2.78217049e-04 2.46861397e-04 4.47824859e-04 6.90782967e-04\n",
      " 1.57074910e-03 9.61848884e-04 8.42340058e-04 4.36466336e-02\n",
      " 1.79428002e-03 7.18845753e-04 3.61154409e-04 5.47469885e-04\n",
      " 4.39023459e-03 2.08284636e-03 8.04030523e-03 1.69470778e-03\n",
      " 9.73760337e-03 3.06461193e-03 2.50089308e-03 0.00000000e+00\n",
      " 4.00628720e-04 4.99677226e-05 3.35098048e-05 0.00000000e+00\n",
      " 3.36108002e-04 4.58864553e-04 5.43842543e-05 0.00000000e+00\n",
      " 5.26291551e-04 1.30617555e-04 1.90471124e-04 0.00000000e+00\n",
      " 3.49891285e-04 0.00000000e+00 0.00000000e+00 4.21429577e-05\n",
      " 8.23279061e-06 0.00000000e+00 0.00000000e+00 3.94018207e-05\n",
      " 2.48050259e-04 0.00000000e+00 1.08986125e-04 1.51742948e-04\n",
      " 2.19355847e-04 7.54217675e-04 6.09948875e-05 9.65957770e-06\n",
      " 2.24916759e-04 0.00000000e+00 6.47711568e-05 1.15972791e-04\n",
      " 0.00000000e+00 5.54523976e-05 2.28981109e-04 2.39948786e-04\n",
      " 3.24043765e-04 0.00000000e+00 9.94668881e-05 5.83781548e-05\n",
      " 4.71907930e-04 6.72383001e-04 1.38001429e-04 5.46521696e-05\n",
      " 1.61102012e-04 2.74890364e-04 9.12017349e-05 6.81535690e-04\n",
      " 0.00000000e+00 4.40394579e-05 1.39547599e-04 6.90275701e-05\n",
      " 0.00000000e+00 1.09967012e-02 2.17819063e-04 0.00000000e+00\n",
      " 1.13999611e-03 0.00000000e+00 1.35274051e-04 2.37011001e-04\n",
      " 1.73438151e-04 0.00000000e+00 1.73293429e-05 2.89862219e-04\n",
      " 1.97108966e-04 0.00000000e+00 1.83672222e-04 4.23085090e-04\n",
      " 2.36929627e-04 0.00000000e+00 0.00000000e+00 2.00751056e-05\n",
      " 1.35763374e-04 0.00000000e+00 0.00000000e+00 4.47569109e-05\n",
      " 0.00000000e+00 1.07971762e-04 2.92520417e-04 3.78015575e-05\n",
      " 5.12948391e-05 1.66336361e-06 0.00000000e+00 0.00000000e+00\n",
      " 2.94000754e-04 2.13557112e-04 1.17722404e-04 5.51251651e-05\n",
      " 1.00806807e-04 0.00000000e+00 0.00000000e+00 7.33929119e-05\n",
      " 2.26294098e-04 4.80104529e-04 1.56503258e-04 7.48275852e-05\n",
      " 1.09912835e-04 0.00000000e+00 1.65254984e-04 0.00000000e+00\n",
      " 1.70687519e-04 1.62258875e-04 1.90064384e-04 2.12244384e-04\n",
      " 1.84164543e-04 0.00000000e+00 9.89053151e-05 2.53974635e-04\n",
      " 3.07352930e-05 0.00000000e+00 1.96745445e-04 1.95749053e-05\n",
      " 7.09753309e-04 4.37820236e-05 0.00000000e+00 2.89744512e-05\n",
      " 1.12449670e-04 4.32992267e-04 0.00000000e+00 4.64914512e-04\n",
      " 1.19088283e-04 1.83089520e-04 0.00000000e+00 1.35412934e-04\n",
      " 4.26836377e-05 4.79733790e-05 3.28571827e-04 1.13037851e-04\n",
      " 2.72432109e-04 1.70684900e-04 0.00000000e+00 1.20059070e-04\n",
      " 8.71103693e-05 0.00000000e+00 2.28123594e-04 4.61381045e-04\n",
      " 8.46770345e-05 1.01229183e-04 0.00000000e+00 4.13914298e-04\n",
      " 4.61947675e-05 3.80646117e-04 0.00000000e+00 5.11253384e-05\n",
      " 0.00000000e+00 0.00000000e+00 2.02572246e-05 0.00000000e+00\n",
      " 9.89592372e-05 1.77754927e-02 1.03484793e-03 6.14309916e-03\n",
      " 4.74339165e-03 6.65501133e-03 8.69715586e-03 0.00000000e+00\n",
      " 1.14382175e-03 4.51047812e-03 1.74666278e-03 4.10898915e-03\n",
      " 3.42474319e-04 1.33575697e-03 1.66561897e-03 1.29418625e-02\n",
      " 3.92396562e-03 1.06365222e-03 6.07594475e-03 1.15930720e-03\n",
      " 1.38547402e-02 5.23670437e-03 2.55682226e-03 4.71916108e-04\n",
      " 1.57841947e-03 1.51107402e-03 5.36408683e-04 1.60773844e-03\n",
      " 8.47981137e-04 4.23023058e-03 6.90059410e-03 2.55812192e-03\n",
      " 1.21000072e-03 8.00320704e-04 4.30421991e-04 1.07528027e-02\n",
      " 1.76857747e-02 6.11393407e-05 4.02049674e-03 6.45354681e-04\n",
      " 3.85797373e-03 1.71012466e-03 2.51775188e-03 2.44090725e-02\n",
      " 1.05056691e-03 2.29118741e-03 2.31783313e-04 7.83019175e-04\n",
      " 3.47426091e-03 3.98960430e-03 2.33827275e-03 2.98912469e-02\n",
      " 2.70877965e-04 1.92155025e-03 4.29180786e-02 2.50570680e-04\n",
      " 3.70227359e-03 6.24482695e-04 7.22377154e-04 3.02979548e-04\n",
      " 2.53101741e-03 3.49196256e-03 9.88384825e-04 5.16557542e-04\n",
      " 0.00000000e+00 2.48358585e-04 1.31417334e-03 2.33818311e-03\n",
      " 1.44514709e-03 4.17526858e-03 5.52928017e-04 3.26980068e-03\n",
      " 6.20736799e-04 7.98071211e-04 3.33623932e-04 5.91928663e-04\n",
      " 4.03003860e-03 4.23705409e-04 5.99740713e-04 4.89265367e-04\n",
      " 0.00000000e+00 9.14020464e-04 5.39456587e-03 1.38731848e-03\n",
      " 5.57731946e-05 0.00000000e+00 1.10081537e-02 1.40249424e-04\n",
      " 7.87582714e-04 1.16955169e-04 2.97796330e-04 0.00000000e+00\n",
      " 1.23415390e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.92618613e-04 6.28957176e-04 9.73675691e-04 1.42873701e-04\n",
      " 2.29432539e-04 3.17064667e-04 3.02286848e-04 1.31869179e-04\n",
      " 4.63956443e-04 1.67133199e-04 1.14525181e-04 8.84993788e-05\n",
      " 3.30187904e-04 4.05702594e-04 7.91903527e-04 1.00020505e-03\n",
      " 2.48594646e-04 6.83153281e-04 3.36183555e-04 2.22723655e-04\n",
      " 0.00000000e+00 1.22057600e-03 2.35644053e-04 5.02825424e-04\n",
      " 1.29439682e-03 2.75513768e-04 1.91986896e-04 1.81889115e-03\n",
      " 1.02911064e-04 4.45041951e-05 1.93820859e-04 1.46476363e-04\n",
      " 3.92745918e-04 2.99248903e-04 5.58553147e-05 2.30135003e-04\n",
      " 5.85352944e-04 1.05917556e-04 0.00000000e+00 0.00000000e+00\n",
      " 1.66467711e-04 7.41280441e-04 1.86512666e-03 8.93120072e-04\n",
      " 0.00000000e+00 4.02141945e-04 0.00000000e+00 0.00000000e+00\n",
      " 4.12908412e-04 2.10823302e-04 0.00000000e+00 1.07414693e-04\n",
      " 7.78032467e-04 0.00000000e+00 1.44339559e-04 0.00000000e+00\n",
      " 0.00000000e+00 9.15460754e-04 0.00000000e+00 4.15732265e-02\n",
      " 5.92916447e-04 7.14758935e-04 1.51113622e-04 0.00000000e+00\n",
      " 2.72282196e-04 2.97317049e-04 4.32262241e-05 0.00000000e+00\n",
      " 2.40117079e-04 7.23801088e-04 6.25666871e-04 9.46684813e-05\n",
      " 1.00287574e-03 1.07588392e-04 1.37554610e-03 0.00000000e+00\n",
      " 5.42943890e-05 1.16446242e-03 7.05234415e-05 1.12320849e-04\n",
      " 9.20625389e-05 2.44583007e-05 4.60250390e-04 2.13659834e-03\n",
      " 9.62858248e-05 0.00000000e+00 2.61312723e-03 4.01715195e-04\n",
      " 2.10885759e-04 0.00000000e+00 9.51565569e-04 2.39909394e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 5.31566271e-04 2.43253991e-04\n",
      " 0.00000000e+00 4.15228633e-03 9.71898029e-04 2.47530453e-03\n",
      " 8.73361714e-03 4.82386397e-03 9.74084018e-04 2.29592944e-04\n",
      " 1.54485504e-04 7.56607260e-05 0.00000000e+00 5.72365592e-04\n",
      " 1.16924100e-04 2.16754992e-03 3.26092297e-04 1.96834677e-04\n",
      " 0.00000000e+00 6.20455900e-03 3.93638591e-04 0.00000000e+00\n",
      " 1.12601214e-04 0.00000000e+00 5.07035620e-05 0.00000000e+00\n",
      " 6.55458774e-04 1.25409992e-04 4.13956805e-05 5.70391676e-05\n",
      " 3.16280784e-04 7.23628254e-05 1.18550954e-04 5.27591365e-05\n",
      " 0.00000000e+00 5.04037889e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.43695477e-04 2.83091562e-04 7.42796983e-05\n",
      " 2.04833079e-04 0.00000000e+00 1.18871173e-03 3.60254577e-04\n",
      " 1.09365629e-03 2.72398698e-04 6.59123994e-04 5.59847045e-04\n",
      " 0.00000000e+00 1.00599747e-04 1.07250649e-04 1.24793427e-04\n",
      " 0.00000000e+00 2.60896195e-04 0.00000000e+00 2.28675359e-04\n",
      " 3.16959748e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 3.18074162e-04 0.00000000e+00 0.00000000e+00 1.23177640e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.79352122e-04 0.00000000e+00 7.63121807e-06 6.01399624e-05\n",
      " 0.00000000e+00 0.00000000e+00 2.97823339e-04 8.86554553e-05\n",
      " 3.54792166e-04 0.00000000e+00 0.00000000e+00 4.95789573e-04\n",
      " 0.00000000e+00 0.00000000e+00 5.23437084e-05 0.00000000e+00\n",
      " 0.00000000e+00 6.11878932e-05 0.00000000e+00 1.33546011e-04\n",
      " 0.00000000e+00 0.00000000e+00 2.06841226e-03 0.00000000e+00\n",
      " 0.00000000e+00 8.52097437e-05 0.00000000e+00 5.44428221e-05\n",
      " 8.32855003e-05 1.76286907e-04 5.11210164e-05 8.83275570e-05\n",
      " 4.20469150e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 2.72457342e-04\n",
      " 3.45024484e-04 1.43884754e-04 5.04482341e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 2.37116287e-03 7.20235766e-06\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.59634335e-04\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.78383076e-05\n",
      " 0.00000000e+00 4.71512787e-04 6.96978605e-05 3.29285115e-03\n",
      " 0.00000000e+00 0.00000000e+00 7.83748738e-03 2.80461769e-04\n",
      " 3.10186791e-04 0.00000000e+00 4.71353676e-04 0.00000000e+00\n",
      " 0.00000000e+00 1.76166504e-05 1.06947031e-04 1.45408252e-04\n",
      " 4.69759078e-04 6.06490998e-04 7.24290498e-04 0.00000000e+00\n",
      " 9.62741105e-05 0.00000000e+00 4.48847350e-05 0.00000000e+00\n",
      " 6.12731499e-04 1.89413768e-04 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 8.67840354e-05 0.00000000e+00 0.00000000e+00\n",
      " 6.53136158e-05 1.45351325e-04 0.00000000e+00 0.00000000e+00\n",
      " 1.36957591e-04 4.35996517e-05 2.12970277e-04 0.00000000e+00\n",
      " 5.46535361e-04 4.64796503e-05 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 2.37803382e-04 0.00000000e+00 4.26423125e-04\n",
      " 8.66521004e-05 0.00000000e+00 1.00538673e-05 2.42251583e-04\n",
      " 2.33902203e-04 7.84204021e-05 0.00000000e+00 0.00000000e+00\n",
      " 7.14398775e-05 0.00000000e+00 0.00000000e+00 4.00897727e-04\n",
      " 1.09103043e-04 0.00000000e+00 5.56786981e-05 4.38095485e-05\n",
      " 0.00000000e+00 1.31324879e-04 3.62367980e-04 0.00000000e+00\n",
      " 0.00000000e+00 3.61947750e-04 0.00000000e+00 3.91058893e-06\n",
      " 0.00000000e+00 0.00000000e+00 2.60861125e-04 6.55309486e-05\n",
      " 0.00000000e+00 8.44836468e-05 8.73289027e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.87394340e-04\n",
      " 0.00000000e+00 5.09383608e-05 1.28969288e-04 7.58282957e-04\n",
      " 2.04214535e-04 0.00000000e+00 6.58371777e-04 4.10490393e-05\n",
      " 0.00000000e+00 2.30848367e-04 1.85491899e-05 2.00283364e-03\n",
      " 3.46410787e-04 6.03373919e-05 1.56492650e-04 0.00000000e+00\n",
      " 7.01112906e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.36275737e-04 0.00000000e+00 5.72049503e-05 9.91872002e-05\n",
      " 8.91346717e-05 0.00000000e+00 4.87112324e-04 0.00000000e+00\n",
      " 0.00000000e+00 1.16670439e-04 0.00000000e+00 0.00000000e+00\n",
      " 7.20749740e-05 0.00000000e+00 0.00000000e+00 3.12558434e-04\n",
      " 2.25603188e-04 1.78749251e-04 0.00000000e+00 1.49184471e-05\n",
      " 1.41413650e-04 0.00000000e+00 0.00000000e+00 4.68220496e-05\n",
      " 5.94016310e-05 1.04461433e-04 0.00000000e+00 5.07054356e-05\n",
      " 1.95332352e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 1.96070934e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 9.15345445e-05 1.16845919e-03 3.01094813e-04\n",
      " 3.70819209e-04 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 2.77383486e-04 7.57754460e-05 7.00169767e-05 0.00000000e+00\n",
      " 0.00000000e+00 7.14034104e-05 8.52123194e-05 0.00000000e+00\n",
      " 8.88914801e-05 2.45094503e-04 5.35839099e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 5.87317511e-04 9.53447248e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 1.09862245e-04 0.00000000e+00 0.00000000e+00\n",
      " 2.07303616e-04 1.68751532e-04 7.81194249e-05 5.24191273e-05\n",
      " 6.81334423e-05 1.76015892e-04 2.10370432e-04 0.00000000e+00\n",
      " 4.96443063e-05 4.69490151e-05 0.00000000e+00 5.43392816e-05\n",
      " 0.00000000e+00 0.00000000e+00 6.18832564e-05 0.00000000e+00\n",
      " 4.74654553e-05 0.00000000e+00 0.00000000e+00 4.56779017e-05\n",
      " 0.00000000e+00 4.47152925e-05 0.00000000e+00 8.96151643e-04\n",
      " 0.00000000e+00 0.00000000e+00 4.41724951e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 3.78369499e-04 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 5.76804923e-05\n",
      " 1.16549032e-04 0.00000000e+00 9.50878821e-05 0.00000000e+00\n",
      " 0.00000000e+00 7.25720602e-05 0.00000000e+00 0.00000000e+00\n",
      " 4.79390110e-05 0.00000000e+00 0.00000000e+00 9.28733643e-05\n",
      " 0.00000000e+00 1.69767300e-05 8.73243116e-05 7.17465809e-05\n",
      " 0.00000000e+00 2.46630574e-04 9.25697823e-05 1.11262496e-04\n",
      " 1.86063902e-04 0.00000000e+00 5.30245889e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.57213209e-05 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 7.66508747e-04\n",
      " 5.48888565e-05 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.07573629e-04\n",
      " 0.00000000e+00 6.68991852e-05 0.00000000e+00 0.00000000e+00\n",
      " 1.33026950e-03 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
      " 0.00000000e+00 0.00000000e+00 4.08219919e-03 4.50557144e-03\n",
      " 2.92695942e-03 3.35401739e-03 4.09100950e-03 2.47267121e-03\n",
      " 7.77299237e-03 1.13715325e-03 2.00110190e-02 1.32051087e-03\n",
      " 5.90628071e-04 7.41138181e-04 1.38821558e-03 1.68722670e-03\n",
      " 4.77470254e-04]\n"
     ]
    }
   ],
   "source": [
    "print(xgbreg.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(xgbreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking\n",
    "gbr1 = GradientBoostingRegressor(loss='huber',\n",
    "                                alpha=0.9,\n",
    "                                validation_fraction=0.2,\n",
    "                                n_iter_no_change=5,\n",
    "                                tol=0.001,\n",
    "                                max_depth=4,\n",
    "                                n_estimators=10000,\n",
    "                                random_state=0,\n",
    "                                verbose=True)\n",
    "gbr2 = GradientBoostingRegressor(loss='huber',\n",
    "                                alpha=0.9,\n",
    "                                validation_fraction=0.2,\n",
    "                                n_iter_no_change=5,\n",
    "                                tol=0.001,\n",
    "                                max_depth=4,\n",
    "                                n_estimators=10000,\n",
    "                                random_state=100,\n",
    "                                verbose=True)\n",
    "gbr3 = GradientBoostingRegressor(loss='huber',\n",
    "                                alpha=0.9,\n",
    "                                validation_fraction=0.2,\n",
    "                                n_iter_no_change=10,\n",
    "                                tol=0.005,\n",
    "                                max_depth=4,\n",
    "                                n_estimators=10000,\n",
    "                                random_state=200,\n",
    "                                verbose=True)\n",
    "gbr4 = GradientBoostingRegressor(loss='huber',\n",
    "                                alpha=0.9,\n",
    "                                validation_fraction=0.2,\n",
    "                                n_iter_no_change=10,\n",
    "                                tol=0.005,\n",
    "                                max_depth=4,\n",
    "                                n_estimators=10000,\n",
    "                                random_state=300,\n",
    "                                verbose=True)\n",
    "xgbr1 = XGBRegressor(booster='gbtree',\n",
    "                      learning_rate=0.04,\n",
    "                      n_estimators=1600,\n",
    "                      min_child_weight=1,\n",
    "                      subsample=1,\n",
    "                      colsample_bytree=0.5, # best\n",
    "                      max_depth=7,\n",
    "                      gamma=5.0,\n",
    "                      random_state=0)\n",
    "xgbr2= XGBRegressor(booster='gbtree',\n",
    "                      learning_rate=0.04,\n",
    "                      n_estimators=1700,\n",
    "                      min_child_weight=1,\n",
    "                      subsample=1,\n",
    "                      colsample_bytree=0.5, # best\n",
    "                      max_depth=7,\n",
    "                      gamma=5.0,\n",
    "                      random_state=200)\n",
    "xgbr3 = XGBRegressor(booster='gbtree',\n",
    "                      learning_rate=0.04,\n",
    "                      n_estimators=1800,\n",
    "                      min_child_weight=1,\n",
    "                      subsample=1,\n",
    "                      colsample_bytree=0.5, # best\n",
    "                      max_depth=7,\n",
    "                      gamma=5.0,\n",
    "                      random_state=400)\n",
    "xgbr4 = XGBRegressor(booster='gbtree',\n",
    "                      learning_rate=0.04,\n",
    "                      n_estimators=1900,\n",
    "                      min_child_weight=1,\n",
    "                      subsample=1,\n",
    "                      colsample_bytree=0.5, # best\n",
    "                      max_depth=7,\n",
    "                      gamma=5.0,\n",
    "                      random_state=600)\n",
    "\n",
    "\n",
    "estimators = [('xgbr1', xgbr1), ('xgbr2', xgbr2), ('xgbr3', xgbr3), ('xgbr4', xgbr4)]\n",
    "stackreg = StackingRegressor(estimators=estimators, verbose=100)\n",
    "stackreg_pipe = Pipeline([('adr_transform', adr_ct), ('stackreg', stackreg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1         952.5937           45.96m\n",
      "         2         896.3714           41.63m\n",
      "         3         848.4243           40.05m\n",
      "         4         808.4645           39.61m\n",
      "         5         757.4050           39.00m\n",
      "         6         713.3501           38.80m\n",
      "         7         670.4756           38.41m\n",
      "         8         638.2463           38.36m\n",
      "         9         615.2928           38.09m\n",
      "        10         591.9321           38.18m\n",
      "        20         439.5372           38.14m\n",
      "        30         367.7854           36.94m\n",
      "        40         326.6351           36.11m\n",
      "        50         294.1300           35.70m\n",
      "        60         267.1857           35.15m\n",
      "        70         246.2243           34.98m\n",
      "        80         230.1154           34.62m\n",
      "        90         219.8640           34.21m\n",
      "       100         211.8309           33.91m\n",
      "       200         166.8786           30.76m\n",
      "       300         147.5202           29.09m\n",
      "       400         137.5783           28.00m\n",
      "       500         131.2452           27.25m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         976.5482           32.26m\n",
      "         2         919.2413           31.65m\n",
      "         3         861.4736           31.34m\n",
      "         4         803.7704           30.94m\n",
      "         5         765.5922           30.60m\n",
      "         6         716.9672           31.12m\n",
      "         7         678.2959           31.01m\n",
      "         8         641.5757           30.78m\n",
      "         9         609.2417           30.83m\n",
      "        10         591.6540           30.60m\n",
      "        20         441.0074           30.52m\n",
      "        30         371.4112           29.74m\n",
      "        40         324.0792           28.97m\n",
      "        50         292.5234           28.58m\n",
      "        60         266.2354           28.14m\n",
      "        70         245.0015           27.87m\n",
      "        80         230.5898           27.54m\n",
      "        90         219.4646           27.11m\n",
      "       100         211.5563           26.71m\n",
      "       200         165.0926           25.02m\n",
      "       300         146.3707           23.78m\n",
      "       400         134.0010           22.96m\n",
      "       500         126.5140           22.26m\n",
      "       600         120.6874           21.83m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         958.7568           33.15m\n",
      "         2         900.6558           32.43m\n",
      "         3         852.0639           32.10m\n",
      "         4         798.0916           32.03m\n",
      "         5         746.9414           31.93m\n",
      "         6         697.5077           31.64m\n",
      "         7         660.4573           31.59m\n",
      "         8         623.8799           31.84m\n",
      "         9         601.9840           31.86m\n",
      "        10         574.2956           31.70m\n",
      "        20         424.0318           31.91m\n",
      "        30         347.5194           31.75m\n",
      "        40         306.1841           30.81m\n",
      "        50         271.2963           30.27m\n",
      "        60         250.5651           29.76m\n",
      "        70         233.8776           29.29m\n",
      "        80         220.9352           28.89m\n",
      "        90         210.0572           28.53m\n",
      "       100         201.3535           28.07m\n",
      "       200         158.9942           25.55m\n",
      "       300         141.2428           24.42m\n",
      "       400         129.3444           23.58m\n",
      "       500         122.7675           22.81m\n",
      "       600         117.1062           22.23m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         941.2620           33.18m\n",
      "         2         884.2651           32.03m\n",
      "         3         836.0577           31.53m\n",
      "         4         777.6608           31.30m\n",
      "         5         742.0620           31.22m\n",
      "         6         698.7599           31.17m\n",
      "         7         656.2414           30.83m\n",
      "         8         623.2062           30.80m\n",
      "         9         600.2093           30.62m\n",
      "        10         581.8078           30.81m\n",
      "        20         423.6051           30.76m\n",
      "        30         356.6221           30.83m\n",
      "        40         314.8806           30.26m\n",
      "        50         282.0792           29.67m\n",
      "        60         260.0802           29.09m\n",
      "        70         242.9440           28.71m\n",
      "        80         225.8437           28.43m\n",
      "        90         215.2865           28.11m\n",
      "       100         205.9326           27.71m\n",
      "       200         162.0297           25.45m\n",
      "       300         143.6591           24.09m\n",
      "       400         132.7607           23.19m\n",
      "       500         126.5345           22.45m\n",
      "       600         120.7315           21.95m\n",
      "       700         116.3020           21.51m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         969.8241           33.93m\n",
      "         2         914.1143           32.51m\n",
      "         3         855.1809           31.99m\n",
      "         4         813.5220           31.47m\n",
      "         5         759.4749           31.15m\n",
      "         6         715.1885           31.24m\n",
      "         7         679.3861           31.06m\n",
      "         8         643.8871           31.12m\n",
      "         9         615.3778           31.02m\n",
      "        10         595.1227           30.87m\n",
      "        20         446.4566           31.31m\n",
      "        30         372.8113           30.77m\n",
      "        40         326.1367           29.74m\n",
      "        50         292.4708           29.18m\n",
      "        60         269.9123           28.76m\n",
      "        70         249.1106           28.41m\n",
      "        80         234.1338           28.05m\n",
      "        90         225.5917           27.54m\n",
      "       100         217.0572           27.23m\n",
      "       200         170.5096           25.06m\n",
      "       300         153.4050           23.70m\n",
      "       400         141.9764           23.04m\n",
      "       500         133.4307           22.54m\n",
      "       600         126.9900           22.02m\n",
      "       700         121.8594           21.58m\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1         930.9356           32.73m\n",
      "         2         876.0794           31.73m\n",
      "         3         830.2309           31.54m\n",
      "         4         774.9006           31.20m\n",
      "         5         740.1204           31.28m\n",
      "         6         698.7527           31.17m\n",
      "         7         656.9293           30.88m\n",
      "         8         625.3372           30.88m\n",
      "         9         593.9511           30.70m\n",
      "        10         571.1944           30.83m\n",
      "        20         425.8178           31.09m\n",
      "        30         357.1649           29.89m\n",
      "        40         312.7464           29.59m\n",
      "        50         283.5177           29.02m\n",
      "        60         260.2878           28.72m\n",
      "        70         246.3298           28.32m\n",
      "        80         228.0405           28.09m\n",
      "        90         217.8770           27.84m\n",
      "       100         208.1042           27.55m\n",
      "       200         161.1887           25.80m\n",
      "       300         143.8754           24.46m\n",
      "       400         133.3234           23.56m\n",
      "       500         125.8043           22.81m\n",
      "       600         119.8367           22.27m\n",
      "0.7417671264791066\n",
      "0.8350629895356667\n",
      "681.435002507303\n",
      "343.1938782144727\n"
     ]
    }
   ],
   "source": [
    "# Execute stack regression pipeline\n",
    "stackreg_pipe.fit(adr_train, train_df['adr'])\n",
    "\n",
    "# Correlation coefficients\n",
    "print(stackreg_pipe.score(adr_train, train_df['adr']))\n",
    "print(stackreg_pipe.score(adr_val, val_df['adr']))\n",
    "\n",
    "# Mean squared errors\n",
    "print(mean_squared_error(stackreg_pipe.predict(adr_train), train_df['adr']))\n",
    "print(mean_squared_error(stackreg_pipe.predict(adr_val), val_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.837, total=  45.8s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   45.8s remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.487, total=  45.5s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.835, total=  45.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.843, total=  46.1s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  3.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.839, total=  46.4s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  3.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.830, total=  45.3s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:  4.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.478, total=  44.8s\n",
      "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:  5.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.826, total=  45.3s\n",
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.825, total=  45.9s\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  6.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.822, total=  45.4s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  7.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.822, total=  45.8s\n",
      "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:  8.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.474, total=  46.1s\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  9.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.815, total=  45.6s\n",
      "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:  9.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.818, total=  46.4s\n",
      "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed: 10.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.817, total=  45.1s\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 11.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.814, total=  45.5s\n",
      "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed: 12.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.468, total=  46.9s\n",
      "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed: 13.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.807, total=  45.3s\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 13.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.810, total=  45.1s\n",
      "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed: 14.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.809, total=  46.3s\n",
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 15.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.852, total=  52.2s\n",
      "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed: 16.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.494, total=  53.7s\n",
      "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed: 17.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.847, total=  52.3s\n",
      "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed: 17.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.850, total=  52.6s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed: 18.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.846, total=  52.6s\n",
      "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed: 19.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.839, total=  46.4s\n",
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed: 20.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.483, total=  46.3s\n",
      "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed: 21.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.834, total=  45.6s\n",
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed: 21.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.833, total=  44.8s\n",
      "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed: 22.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.834, total=  45.4s\n",
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 23.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.834, total=  45.7s\n",
      "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed: 24.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.479, total=  46.7s\n",
      "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed: 25.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.827, total=  45.7s\n",
      "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed: 25.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.828, total=  46.2s\n",
      "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed: 26.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.828, total=  46.2s\n",
      "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed: 27.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.825, total=  46.2s\n",
      "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed: 28.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.475, total=  46.4s\n",
      "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed: 28.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.820, total=  46.6s\n",
      "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed: 29.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.822, total=  46.0s\n",
      "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed: 30.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.6, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.821, total=  47.6s\n",
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 31.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.832, total=  49.8s\n",
      "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed: 32.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.487, total=  50.5s\n",
      "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed: 32.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.835, total=  49.8s\n",
      "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed: 33.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.842, total=  49.0s\n",
      "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed: 34.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.841, total=  50.3s\n",
      "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed: 35.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.829, total=  49.1s\n",
      "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed: 36.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.480, total=  50.0s\n",
      "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed: 37.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.827, total=  49.1s\n",
      "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed: 37.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.825, total=  49.6s\n",
      "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed: 38.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.827, total=  49.9s\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 39.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.823, total=  50.0s\n",
      "[Parallel(n_jobs=1)]: Done  51 out of  51 | elapsed: 40.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.476, total=  50.0s\n",
      "[Parallel(n_jobs=1)]: Done  52 out of  52 | elapsed: 41.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.820, total=  49.2s\n",
      "[Parallel(n_jobs=1)]: Done  53 out of  53 | elapsed: 41.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.819, total=  49.7s\n",
      "[Parallel(n_jobs=1)]: Done  54 out of  54 | elapsed: 42.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.818, total=  50.4s\n",
      "[Parallel(n_jobs=1)]: Done  55 out of  55 | elapsed: 43.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.817, total=  50.3s\n",
      "[Parallel(n_jobs=1)]: Done  56 out of  56 | elapsed: 44.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.471, total=  49.6s\n",
      "[Parallel(n_jobs=1)]: Done  57 out of  57 | elapsed: 45.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.809, total=  49.6s\n",
      "[Parallel(n_jobs=1)]: Done  58 out of  58 | elapsed: 46.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.812, total=  50.0s\n",
      "[Parallel(n_jobs=1)]: Done  59 out of  59 | elapsed: 46.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.813, total=  50.4s\n",
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed: 47.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.845, total=  57.5s\n",
      "[Parallel(n_jobs=1)]: Done  61 out of  61 | elapsed: 48.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.491, total=  57.4s\n",
      "[Parallel(n_jobs=1)]: Done  62 out of  62 | elapsed: 49.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.842, total=  57.6s\n",
      "[Parallel(n_jobs=1)]: Done  63 out of  63 | elapsed: 50.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.853, total=  57.1s\n",
      "[Parallel(n_jobs=1)]: Done  64 out of  64 | elapsed: 51.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.847, total=  57.5s\n",
      "[Parallel(n_jobs=1)]: Done  65 out of  65 | elapsed: 52.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.839, total=  58.1s\n",
      "[Parallel(n_jobs=1)]: Done  66 out of  66 | elapsed: 53.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.485, total=  56.4s\n",
      "[Parallel(n_jobs=1)]: Done  67 out of  67 | elapsed: 54.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.836, total=  58.0s\n",
      "[Parallel(n_jobs=1)]: Done  68 out of  68 | elapsed: 55.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.835, total=  57.2s\n",
      "[Parallel(n_jobs=1)]: Done  69 out of  69 | elapsed: 56.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.836, total=  57.6s\n",
      "[Parallel(n_jobs=1)]: Done  70 out of  70 | elapsed: 57.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.836, total=  58.2s\n",
      "[Parallel(n_jobs=1)]: Done  71 out of  71 | elapsed: 58.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.482, total=  58.5s\n",
      "[Parallel(n_jobs=1)]: Done  72 out of  72 | elapsed: 59.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.828, total=  60.0s\n",
      "[Parallel(n_jobs=1)]: Done  73 out of  73 | elapsed: 60.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.828, total=  58.7s\n",
      "[Parallel(n_jobs=1)]: Done  74 out of  74 | elapsed: 61.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.832, total=  58.6s\n",
      "[Parallel(n_jobs=1)]: Done  75 out of  75 | elapsed: 62.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.827, total=  59.2s\n",
      "[Parallel(n_jobs=1)]: Done  76 out of  76 | elapsed: 63.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.479, total=  58.1s\n",
      "[Parallel(n_jobs=1)]: Done  77 out of  77 | elapsed: 64.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.821, total=  58.2s\n",
      "[Parallel(n_jobs=1)]: Done  78 out of  78 | elapsed: 65.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.823, total=  57.5s\n",
      "[Parallel(n_jobs=1)]: Done  79 out of  79 | elapsed: 66.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=0.8, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.822, total=  58.6s\n",
      "[Parallel(n_jobs=1)]: Done  80 out of  80 | elapsed: 67.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.838, total=  53.2s\n",
      "[Parallel(n_jobs=1)]: Done  81 out of  81 | elapsed: 68.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.484, total=  54.5s\n",
      "[Parallel(n_jobs=1)]: Done  82 out of  82 | elapsed: 68.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.837, total=  54.0s\n",
      "[Parallel(n_jobs=1)]: Done  83 out of  83 | elapsed: 69.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.840, total=  53.1s\n",
      "[Parallel(n_jobs=1)]: Done  84 out of  84 | elapsed: 70.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.839, total=  54.0s\n",
      "[Parallel(n_jobs=1)]: Done  85 out of  85 | elapsed: 71.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.832, total=  53.5s\n",
      "[Parallel(n_jobs=1)]: Done  86 out of  86 | elapsed: 72.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.479, total=  55.1s\n",
      "[Parallel(n_jobs=1)]: Done  87 out of  87 | elapsed: 73.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.826, total=  53.5s\n",
      "[Parallel(n_jobs=1)]: Done  88 out of  88 | elapsed: 74.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.829, total=  54.0s\n",
      "[Parallel(n_jobs=1)]: Done  89 out of  89 | elapsed: 75.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.826, total=  53.4s\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed: 76.1min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.824, total=  53.6s\n",
      "[Parallel(n_jobs=1)]: Done  91 out of  91 | elapsed: 77.0min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.477, total=  55.4s\n",
      "[Parallel(n_jobs=1)]: Done  92 out of  92 | elapsed: 77.9min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.823, total=  53.8s\n",
      "[Parallel(n_jobs=1)]: Done  93 out of  93 | elapsed: 78.8min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.819, total=  54.2s\n",
      "[Parallel(n_jobs=1)]: Done  94 out of  94 | elapsed: 79.7min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.821, total=  53.5s\n",
      "[Parallel(n_jobs=1)]: Done  95 out of  95 | elapsed: 80.6min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.818, total=  54.3s\n",
      "[Parallel(n_jobs=1)]: Done  96 out of  96 | elapsed: 81.5min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.473, total=  55.0s\n",
      "[Parallel(n_jobs=1)]: Done  97 out of  97 | elapsed: 82.4min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.811, total=  54.7s\n",
      "[Parallel(n_jobs=1)]: Done  98 out of  98 | elapsed: 83.3min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.813, total=  53.9s\n",
      "[Parallel(n_jobs=1)]: Done  99 out of  99 | elapsed: 84.2min remaining:    0.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=6, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.813, total=  54.0s\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.854, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.493, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.839, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.846, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=0, score=0.844, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.842, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.484, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.835, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.834, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=10, score=0.836, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.835, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.482, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.832, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.831, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=20, score=0.829, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.830, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.480, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.826, total= 1.1min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.823, total= 1.0min\n",
      "[CV] xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30 \n",
      "[CV]  xgbreg__colsample_bytree=1.0, xgbreg__max_depth=7, xgbreg__n_estimators=3000, xgbreg__reg_lambda=30, score=0.825, total= 1.2min\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed: 106.3min finished\n",
      "{'xgbreg__colsample_bytree': 0.6, 'xgbreg__max_depth': 7, 'xgbreg__n_estimators': 3000, 'xgbreg__reg_lambda': 0}\n",
      "0.7776700825441709\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       45.282498      0.470860         0.485391        0.006627   \n",
      "1       44.864621      0.322664         0.492918        0.027134   \n",
      "2       45.310530      0.439589         0.484980        0.006247   \n",
      "3       45.318806      0.702828         0.517457        0.059105   \n",
      "4       52.137676      0.544314         0.527265        0.005953   \n",
      "5       45.205950      0.586766         0.514911        0.003410   \n",
      "6       45.575628      0.383959         0.524824        0.004568   \n",
      "7       46.025925      0.564237         0.536766        0.005962   \n",
      "8       49.388349      0.518166         0.484913        0.007864   \n",
      "9       49.052411      0.392639         0.473531        0.005540   \n",
      "10      49.384677      0.432419         0.476923        0.002888   \n",
      "11      49.486006      0.336311         0.481274        0.003657   \n",
      "12      56.911365      0.155650         0.523252        0.005076   \n",
      "13      56.961134      0.591382         0.505023        0.005700   \n",
      "14      58.283991      0.618329         0.518867        0.003211   \n",
      "15      57.804067      0.563922         0.523655        0.006167   \n",
      "16      53.281406      0.524052         0.480155        0.002467   \n",
      "17      53.422201      0.620255         0.465646        0.006750   \n",
      "18      53.637157      0.710945         0.466248        0.003397   \n",
      "19      53.908833      0.436056         0.468864        0.003365   \n",
      "20      62.518395      0.650280         0.520862        0.009537   \n",
      "21      62.056325      0.434620         0.494725        0.003224   \n",
      "22      62.700565      0.680838         0.520338        0.031119   \n",
      "23      65.340576      2.663449         0.502988        0.001699   \n",
      "\n",
      "   param_xgbreg__colsample_bytree param_xgbreg__max_depth  \\\n",
      "0                             0.6                       6   \n",
      "1                             0.6                       6   \n",
      "2                             0.6                       6   \n",
      "3                             0.6                       6   \n",
      "4                             0.6                       7   \n",
      "5                             0.6                       7   \n",
      "6                             0.6                       7   \n",
      "7                             0.6                       7   \n",
      "8                             0.8                       6   \n",
      "9                             0.8                       6   \n",
      "10                            0.8                       6   \n",
      "11                            0.8                       6   \n",
      "12                            0.8                       7   \n",
      "13                            0.8                       7   \n",
      "14                            0.8                       7   \n",
      "15                            0.8                       7   \n",
      "16                              1                       6   \n",
      "17                              1                       6   \n",
      "18                              1                       6   \n",
      "19                              1                       6   \n",
      "20                              1                       7   \n",
      "21                              1                       7   \n",
      "22                              1                       7   \n",
      "23                              1                       7   \n",
      "\n",
      "   param_xgbreg__n_estimators param_xgbreg__reg_lambda  \\\n",
      "0                        3000                        0   \n",
      "1                        3000                       10   \n",
      "2                        3000                       20   \n",
      "3                        3000                       30   \n",
      "4                        3000                        0   \n",
      "5                        3000                       10   \n",
      "6                        3000                       20   \n",
      "7                        3000                       30   \n",
      "8                        3000                        0   \n",
      "9                        3000                       10   \n",
      "10                       3000                       20   \n",
      "11                       3000                       30   \n",
      "12                       3000                        0   \n",
      "13                       3000                       10   \n",
      "14                       3000                       20   \n",
      "15                       3000                       30   \n",
      "16                       3000                        0   \n",
      "17                       3000                       10   \n",
      "18                       3000                       20   \n",
      "19                       3000                       30   \n",
      "20                       3000                        0   \n",
      "21                       3000                       10   \n",
      "22                       3000                       20   \n",
      "23                       3000                       30   \n",
      "\n",
      "                                               params  split0_test_score  \\\n",
      "0   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.836550   \n",
      "1   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.830405   \n",
      "2   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.822116   \n",
      "3   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.813656   \n",
      "4   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.851938   \n",
      "5   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.838790   \n",
      "6   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.834368   \n",
      "7   {'xgbreg__colsample_bytree': 0.6, 'xgbreg__max...           0.825395   \n",
      "8   {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.831634   \n",
      "9   {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.829221   \n",
      "10  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.823160   \n",
      "11  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.816648   \n",
      "12  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.844566   \n",
      "13  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.839277   \n",
      "14  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.835557   \n",
      "15  {'xgbreg__colsample_bytree': 0.8, 'xgbreg__max...           0.826558   \n",
      "16  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.838032   \n",
      "17  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.832267   \n",
      "18  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.824137   \n",
      "19  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.818340   \n",
      "20  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.854379   \n",
      "21  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.841842   \n",
      "22  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.834982   \n",
      "23  {'xgbreg__colsample_bytree': 1.0, 'xgbreg__max...           0.829855   \n",
      "\n",
      "    split1_test_score  split2_test_score  split3_test_score  \\\n",
      "0            0.486793           0.834608           0.842649   \n",
      "1            0.478365           0.826341           0.824932   \n",
      "2            0.473826           0.815256           0.818250   \n",
      "3            0.467943           0.806923           0.810168   \n",
      "4            0.493543           0.846916           0.849969   \n",
      "5            0.483272           0.834267           0.833077   \n",
      "6            0.479112           0.826690           0.827913   \n",
      "7            0.475358           0.820206           0.822040   \n",
      "8            0.487332           0.835050           0.842296   \n",
      "9            0.479793           0.827294           0.824803   \n",
      "10           0.476353           0.820152           0.819211   \n",
      "11           0.470942           0.808955           0.812001   \n",
      "12           0.490835           0.842269           0.853367   \n",
      "13           0.485209           0.835684           0.834974   \n",
      "14           0.482248           0.827857           0.827617   \n",
      "15           0.479260           0.820650           0.823329   \n",
      "16           0.484330           0.836815           0.839854   \n",
      "17           0.478742           0.826349           0.828687   \n",
      "18           0.477196           0.822501           0.818700   \n",
      "19           0.472587           0.811031           0.812999   \n",
      "20           0.493107           0.838800           0.846137   \n",
      "21           0.484225           0.835489           0.833566   \n",
      "22           0.482372           0.832107           0.830802   \n",
      "23           0.480156           0.826215           0.823291   \n",
      "\n",
      "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
      "0            0.839051         0.767930        0.140594                4  \n",
      "1            0.822018         0.756412        0.139050               16  \n",
      "2            0.816701         0.749230        0.137721               21  \n",
      "3            0.809425         0.741623        0.136857               24  \n",
      "4            0.845985         0.777670        0.142080                1  \n",
      "5            0.833830         0.764647        0.140702                9  \n",
      "6            0.827710         0.759159        0.140049               12  \n",
      "7            0.821206         0.752841        0.138752               18  \n",
      "8            0.841123         0.767487        0.140132                6  \n",
      "9            0.827109         0.757644        0.138933               14  \n",
      "10           0.818036         0.751383        0.137525               20  \n",
      "11           0.813465         0.744402        0.136752               23  \n",
      "12           0.846912         0.775590        0.142426                2  \n",
      "13           0.835944         0.766218        0.140512                7  \n",
      "14           0.831770         0.761010        0.139411               11  \n",
      "15           0.822255         0.754410        0.137589               17  \n",
      "16           0.839202         0.767647        0.141662                5  \n",
      "17           0.825656         0.758340        0.139818               13  \n",
      "18           0.820583         0.752623        0.137726               19  \n",
      "19           0.812614         0.745514        0.136486               22  \n",
      "20           0.844014         0.775287        0.141179                3  \n",
      "21           0.835879         0.766200        0.141015                8  \n",
      "22           0.829460         0.761945        0.139798               10  \n",
      "23           0.824708         0.756845        0.138362               15  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation for regression\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=7777)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'xgbreg__n_estimators': [3000],\n",
    "    #'xgbreg__learning_rate': [0.01],\n",
    "    'xgbreg__reg_lambda': [0, 10, 20, 30],\n",
    "    'xgbreg__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'xgbreg__max_depth': [6, 7]\n",
    "}\n",
    "\n",
    "raw_adr_train = raw_train_df[adr_featureCols]\n",
    "gs = GridSearchCV(xgbreg_pipe, param_grid, cv=kf, verbose=100, n_jobs=1)\n",
    "gs.fit(raw_adr_train, raw_train_df['adr'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 2: Predict is_canceled by soft logistic regression\n",
    "# Determine features\n",
    "isc_numericCols = ['lead_time', 'arrival_date_year', 'stays', 'stays_in_weekend_nights', 'stays_in_week_nights',\\\n",
    "               'adults', 'children', 'babies', 'persons', 'previous_cancellations',\\\n",
    "               'previous_bookings_not_canceled', 'booking_changes', 'days_in_waiting_list',\\\n",
    "               'required_car_parking_spaces', 'total_of_special_requests'\n",
    "              ]\n",
    "isc_categoricCols = ['hotel', 'arrival_date_month',\\\n",
    "                'arrival_date_week_number', 'arrival_date_day_of_month', 'meal',\\\n",
    "                'country', 'market_segment', 'distribution_channel',\\\n",
    "                'is_repeated_guest', 'reserved_room_type', 'assigned_room_type',\\\n",
    "                'deposit_type', 'customer_type', 'company', 'agent'\n",
    "               ]\n",
    "isc_featureCols = isc_numericCols + isc_categoricCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "isc_train = train_df[isc_featureCols]\n",
    "isc_val = val_df[isc_featureCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining both categorical and numerical column transformations\n",
    "isc_ct = ColumnTransformer(transformers=[('cat', cat_pipe, isc_categoricCols), ('num', num_pipe, isc_numericCols)])\n",
    "isc_train_transformed = isc_ct.fit_transform(isc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "isc_sel = VarianceThreshold(threshold=(.9 * (1 - .9)))\n",
    "isc_svd = TruncatedSVD(n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logostic regression\n",
    "logistic = LogisticRegression(penalty='l2',\n",
    "                              C=10.0,\n",
    "                              max_iter=10000)\n",
    "\n",
    "# Build pipeline\n",
    "logistic_pipe = Pipeline([('isc_transform', isc_ct), ('logistic', logistic)], verbose=100)\n",
    "\n",
    "# Decide sample weight\n",
    "sample_weights = train_df.apply(lambda row: pow(row.expected_cost, 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.304021611637356\n",
      "6.029936608481809\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "logistic_pipe.fit(isc_train, train_df['is_canceled'], logistic__sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logistic__C': 10}\n",
      "-0.3380959369760162\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       3.748292      0.062424         0.218358        0.008583   \n",
      "1      14.137441      0.197254         0.211806        0.008511   \n",
      "2      17.159322      0.360976         0.211819        0.004099   \n",
      "\n",
      "  param_logistic__C                params  split0_test_score  \\\n",
      "0               0.1  {'logistic__C': 0.1}          -0.343959   \n",
      "1                10   {'logistic__C': 10}          -0.340535   \n",
      "2                25   {'logistic__C': 25}          -0.341172   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0          -0.343697          -0.337794        -0.341817        0.002847   \n",
      "1          -0.339546          -0.334207        -0.338096        0.002779   \n",
      "2          -0.340410          -0.334687        -0.338756        0.002894   \n",
      "\n",
      "   rank_test_score  \n",
      "0                3  \n",
      "1                1  \n",
      "2                2  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'logistic__C': [0.1, 10, 25]\n",
    "}\n",
    "\n",
    "raw_isc_train = raw_train_df[isc_featureCols]\n",
    "gs = GridSearchCV(logistic_pipe, param_grid, cv=kf, scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n",
    "gs.fit(raw_isc_train, raw_train_df['is_canceled'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient boosting classifier\n",
    "gbc = GradientBoostingClassifier(validation_fraction=0.2,\n",
    "                                 n_iter_no_change=10,\n",
    "                                 tol=0.001,\n",
    "                                 n_estimators=10000,\n",
    "                                 max_depth=3,\n",
    "                                 subsample=0.8,\n",
    "                                 random_state=1126)\n",
    "\n",
    "# Build pipeline\n",
    "gbc_pipe = Pipeline([('isc_transform', isc_ct), ('gbc', gbc)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.109562019380921\n",
      "4.597047650071323\n"
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "gbc_pipe.fit(isc_train, train_df['is_canceled'])\n",
    "\n",
    "# Cross entropy loss\n",
    "print(log_loss(train_df['is_canceled'], gbc_pipe.predict(isc_train)))\n",
    "print(log_loss(val_df['is_canceled'], gbc_pipe.predict(isc_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gbc__n_iter_no_change': 20, 'gbc__subsample': 0.8}\n",
      "-0.2716092829674563\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0      26.408494      1.263403         0.328480        0.005972   \n",
      "1      30.386611      4.142013         0.347515        0.023091   \n",
      "2      26.846646      2.423162         0.324061        0.013812   \n",
      "3      42.969898      7.979902         0.407731        0.038751   \n",
      "4      49.400149      1.799360         0.436640        0.010284   \n",
      "5      48.186269      1.470150         0.424074        0.003629   \n",
      "\n",
      "  param_gbc__n_iter_no_change param_gbc__subsample  \\\n",
      "0                          10                  0.6   \n",
      "1                          10                  0.8   \n",
      "2                          10                    1   \n",
      "3                          20                  0.6   \n",
      "4                          20                  0.8   \n",
      "5                          20                    1   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'gbc__n_iter_no_change': 10, 'gbc__subsample'...          -0.284171   \n",
      "1  {'gbc__n_iter_no_change': 10, 'gbc__subsample'...          -0.286650   \n",
      "2  {'gbc__n_iter_no_change': 10, 'gbc__subsample'...          -0.285970   \n",
      "3  {'gbc__n_iter_no_change': 20, 'gbc__subsample'...          -0.285067   \n",
      "4  {'gbc__n_iter_no_change': 20, 'gbc__subsample'...          -0.271965   \n",
      "5  {'gbc__n_iter_no_change': 20, 'gbc__subsample'...          -0.277793   \n",
      "\n",
      "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0          -0.286813          -0.278652        -0.283212        0.003400   \n",
      "1          -0.280617          -0.277055        -0.281440        0.003960   \n",
      "2          -0.290128          -0.283564        -0.286554        0.002711   \n",
      "3          -0.273385          -0.268011        -0.275488        0.007120   \n",
      "4          -0.274756          -0.268107        -0.271609        0.002726   \n",
      "5          -0.277694          -0.271862        -0.275783        0.002773   \n",
      "\n",
      "   rank_test_score  \n",
      "0                5  \n",
      "1                4  \n",
      "2                6  \n",
      "3                2  \n",
      "4                1  \n",
      "5                3  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'gbc__learning_rate': [0.01, 0.05, 1.0],\n",
    "    'gbc__n_iter_no_change': [20],\n",
    "    'gbc__subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "raw_isc_train = raw_train_df[isc_featureCols]\n",
    "gs = GridSearchCV(gbc_pipe, param_grid, cv=kf, scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True), verbose=100)\n",
    "gs.fit(raw_isc_train, raw_train_df['is_canceled'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest classifier\n",
    "rfc = RandomForestClassifier(n_estimators=500,\n",
    "                             max_depth=None,\n",
    "                             max_features='sqrt',\n",
    "                             random_state=1126)\n",
    "rfc_pipe = Pipeline([('isc_transform', isc_ct), ('rfc', rfc)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a9f2c623483a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Execute pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrfc_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_canceled'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 392\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Execute pipeline\n",
    "rfc_pipe.fit(isc_train, train_df['is_canceled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] rfc__max_depth=None, rfc__n_estimators=500 ......................\n",
      "[CV]  rfc__max_depth=None, rfc__n_estimators=500, score=-0.241, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.4min remaining:    0.0s\n",
      "[CV] rfc__max_depth=None, rfc__n_estimators=500 ......................\n",
      "[CV]  rfc__max_depth=None, rfc__n_estimators=500, score=-0.245, total= 3.3min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.8min remaining:    0.0s\n",
      "[CV] rfc__max_depth=None, rfc__n_estimators=500 ......................\n",
      "[CV]  rfc__max_depth=None, rfc__n_estimators=500, score=-0.239, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.1min remaining:    0.0s\n",
      "[CV] rfc__max_depth=None, rfc__n_estimators=500 ......................\n",
      "[CV]  rfc__max_depth=None, rfc__n_estimators=500, score=-0.239, total= 3.3min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 13.5min remaining:    0.0s\n",
      "[CV] rfc__max_depth=None, rfc__n_estimators=500 ......................\n",
      "[CV]  rfc__max_depth=None, rfc__n_estimators=500, score=-0.239, total= 3.3min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 16.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 16.8min finished\n",
      "{'rfc__max_depth': None, 'rfc__n_estimators': 500}\n",
      "-0.24065610202418472\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0     201.043926      2.012688         0.824361         0.00287   \n",
      "\n",
      "  param_rfc__max_depth param_rfc__n_estimators  \\\n",
      "0                 None                     500   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'rfc__max_depth': None, 'rfc__n_estimators': ...          -0.241201   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0          -0.245329          -0.238945          -0.238542          -0.239264   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0        -0.240656        0.002509                1  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'rfc__max_depth': [None],\n",
    "    'rfc__n_estimators': [500]\n",
    "}\n",
    "\n",
    "raw_isc_train = raw_train_df[isc_featureCols]\n",
    "gs = GridSearchCV(rfc_pipe, param_grid, cv=kf, scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True), verbose=100)\n",
    "gs.fit(raw_isc_train, raw_train_df['is_canceled'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost classifier\n",
    "xgbc = XGBClassifier(booster='gbtree',\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=0.04,\n",
    "                     n_estimators=2500,\n",
    "                     subsample=0.5,\n",
    "                     colsample_bytree=0.6, # best\n",
    "                     max_depth=3,\n",
    "                     gamma=1.0,\n",
    "                     reg_lambda=1.0,\n",
    "                     random_state=1126)\n",
    "xgbc_pipe = Pipeline([('isc_transform', isc_ct), ('xgbc', xgbc)], verbose=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:59:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "3.5053665469546416\n",
      "4.385307875453428\n"
     ]
    }
   ],
   "source": [
    "# Execute XGBoost pipeline\n",
    "xgbc_pipe.fit(isc_train, train_df['is_canceled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[CV] xgbc__learning_rate=0.04, xgbc__subsample=0.5 ...................\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:56:25] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  32.1s\n",
      "[CV]  xgbc__learning_rate=0.04, xgbc__subsample=0.5, score=-0.254, total=  33.4s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.4s remaining:    0.0s\n",
      "[CV] xgbc__learning_rate=0.04, xgbc__subsample=0.5 ...................\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:56:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  33.1s\n",
      "[CV]  xgbc__learning_rate=0.04, xgbc__subsample=0.5, score=-0.254, total=  34.1s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[CV] xgbc__learning_rate=0.04, xgbc__subsample=0.5 ...................\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:57:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  32.8s\n",
      "[CV]  xgbc__learning_rate=0.04, xgbc__subsample=0.5, score=-0.251, total=  33.7s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.7min remaining:    0.0s\n",
      "[CV] xgbc__learning_rate=0.04, xgbc__subsample=0.5 ...................\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:58:06] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  32.8s\n",
      "[CV]  xgbc__learning_rate=0.04, xgbc__subsample=0.5, score=-0.249, total=  33.8s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.3min remaining:    0.0s\n",
      "[CV] xgbc__learning_rate=0.04, xgbc__subsample=0.5 ...................\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:58:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  32.8s\n",
      "[CV]  xgbc__learning_rate=0.04, xgbc__subsample=0.5, score=-0.251, total=  33.8s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min finished\n",
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[08:59:14] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[Pipeline] .............. (step 2 of 2) Processing xgbc, total=  39.4s\n",
      "{'xgbc__learning_rate': 0.04, 'xgbc__subsample': 0.5}\n",
      "-0.2517489330402073\n",
      "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0      33.390088      0.236886         0.379101         0.01689   \n",
      "\n",
      "  param_xgbc__learning_rate param_xgbc__subsample  \\\n",
      "0                      0.04                   0.5   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0  {'xgbc__learning_rate': 0.04, 'xgbc__subsample...          -0.254428   \n",
      "\n",
      "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
      "0          -0.254067          -0.250626          -0.248538          -0.251086   \n",
      "\n",
      "   mean_test_score  std_test_score  rank_test_score  \n",
      "0        -0.251749        0.002217                1  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'xgbc__learning_rate': [0.04],\n",
    "    'xgbc__subsample': [0.5]\n",
    "}\n",
    "\n",
    "raw_isc_train = raw_train_df[isc_featureCols]\n",
    "gs = GridSearchCV(xgbc_pipe, param_grid, cv=kf, scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True), verbose=100)\n",
    "gs.fit(raw_isc_train, raw_train_df['is_canceled'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack classifier\n",
    "xgbc1 = XGBClassifier(booster='gbtree',\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=0.01,\n",
    "                     n_estimators=1500,\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8, # best\n",
    "                     max_depth=6,\n",
    "                     gamma=0.0,\n",
    "                     random_state=0)\n",
    "\n",
    "xgbc2 = XGBClassifier(booster='gbtree',\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=0.01,\n",
    "                     n_estimators=2000,\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.7, # best\n",
    "                     max_depth=6,\n",
    "                     gamma=0.0,\n",
    "                     random_state=100)\n",
    "\n",
    "xgbc3 = XGBClassifier(booster='gbtree',\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=0.01,\n",
    "                     n_estimators=2500,\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.8, # best\n",
    "                     max_depth=6,\n",
    "                     gamma=3.5,\n",
    "                     random_state=200)\n",
    "\n",
    "xgbc4 = XGBClassifier(booster='gbtree',\n",
    "                     objective='binary:logistic',\n",
    "                     learning_rate=0.01,\n",
    "                     n_estimators=3000,\n",
    "                     min_child_weight=1,\n",
    "                     subsample=0.8,\n",
    "                     colsample_bytree=0.9, # best\n",
    "                     max_depth=6,\n",
    "                     gamma=3.5,\n",
    "                     random_state=300)\n",
    "\n",
    "rfc1 = RandomForestClassifier(n_estimators=500,\n",
    "                             max_depth=None,\n",
    "                             max_features='sqrt',\n",
    "                             random_state=400,\n",
    "                             n_jobs=4)\n",
    "\n",
    "rfc2 = RandomForestClassifier(n_estimators=500,\n",
    "                             max_depth=None,\n",
    "                             max_features='sqrt',\n",
    "                             random_state=500,\n",
    "                             n_jobs=4)\n",
    "\n",
    "gbc1 = GradientBoostingClassifier(validation_fraction=0.2,\n",
    "                                 n_iter_no_change=10,\n",
    "                                 tol=0.001,\n",
    "                                 n_estimators=10000,\n",
    "                                 random_state=600)\n",
    "\n",
    "gbc2 = GradientBoostingClassifier(validation_fraction=0.2,\n",
    "                                 n_iter_no_change=10,\n",
    "                                 tol=0.001,\n",
    "                                 n_estimators=10000,\n",
    "                                 random_state=700)\n",
    "\n",
    "gbc3 = GradientBoostingClassifier(validation_fraction=0.2,\n",
    "                                 n_iter_no_change=20,\n",
    "                                 tol=0.005,\n",
    "                                 n_estimators=10000,\n",
    "                                 random_state=800)\n",
    "\n",
    "gbc4 = GradientBoostingClassifier(validation_fraction=0.2,\n",
    "                                 n_iter_no_change=20,\n",
    "                                 tol=0.005,\n",
    "                                 n_estimators=10000,\n",
    "                                 random_state=900)\n",
    "\n",
    "logistic1 = LogisticRegression(penalty='l2',\n",
    "                              C=90.0,\n",
    "                              max_iter=10000)\n",
    "\n",
    "logistic2 = LogisticRegression(penalty='l2',\n",
    "                              C=70.0,\n",
    "                              max_iter=8000)\n",
    "\n",
    "estimators = [('xgbc1', xgbc1), ('xgbc2', xgbc2), ('xgbc3', xgbc3), ('xgbc4', xgbc4),\n",
    "              ('rfc1', rfc1), ('rfc2', rfc2),\n",
    "              ('gbc1', gbc1), ('gbc2', gbc2), ('gbc3', gbc3), ('gbc4', gbc4)]\n",
    "stackc = StackingClassifier(estimators=estimators, verbose=100)\n",
    "stackc_pipe = Pipeline([('isc_transform', isc_ct), ('stackc', stackc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4770479687499551\n",
      "1.4668296178390405\n"
     ]
    }
   ],
   "source": [
    "# Execute stackc pipeline\n",
    "stackc_pipe.fit(isc_train, train_df['is_canceled'])\n",
    "\n",
    "# Cross entropy loss\n",
    "print(log_loss(stackc_pipe.predict(isc_train), train_df['is_canceled']))\n",
    "print(log_loss(stackc_pipe.predict(isc_val), val_df['is_canceled']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:16:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:16:32] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:16:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:17:23] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:00] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:18:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:19:15] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:20:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:20:52] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:21:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:22:40] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:23:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:24:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:25:05] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:25:31] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:25:57] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:26:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:27:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:27:48] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:28:36] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:29:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:30:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:31:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:32:11] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:33:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:34:02] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:34:28] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:04] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:35:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:36:19] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:37:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:37:55] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:38:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:39:43] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:40:42] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:41:41] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:42:07] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:42:33] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:42:58] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:43:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:44:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:44:49] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:45:37] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:46:24] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:47:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:48:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:49:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:892: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:50:12] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'xgbc__gamma': 2.5, 'xgbc__n_estimators': 2500}\n",
      "-0.2541087412398761\n",
      "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
      "0       24.870938      0.199448         0.712743        0.039753   \n",
      "1       36.259700      0.279511         0.806287        0.057031   \n",
      "2       47.677677      0.167785         0.884678        0.078864   \n",
      "3       58.615890      0.553827         0.928959        0.064682   \n",
      "4       25.211329      0.077153         0.701996        0.017052   \n",
      "5       36.425261      0.370019         0.761870        0.007250   \n",
      "6       47.338361      0.403701         0.816493        0.004352   \n",
      "7       58.698398      0.665474         0.872329        0.012453   \n",
      "8       24.786041      0.361252         0.676111        0.001238   \n",
      "9       36.171772      0.648569         0.752358        0.002150   \n",
      "10      47.364672      0.238556         0.814148        0.004893   \n",
      "11      58.459388      0.239972         0.892255        0.043124   \n",
      "12      24.915874      0.365532         0.714098        0.039517   \n",
      "13      36.126838      0.446169         0.779543        0.039259   \n",
      "14      47.090810      0.457255         0.809107        0.007190   \n",
      "15      58.592048      0.145742         0.890052        0.043538   \n",
      "\n",
      "   param_xgbc__gamma param_xgbc__n_estimators  \\\n",
      "0                2.5                     1000   \n",
      "1                2.5                     1500   \n",
      "2                2.5                     2000   \n",
      "3                2.5                     2500   \n",
      "4                  3                     1000   \n",
      "5                  3                     1500   \n",
      "6                  3                     2000   \n",
      "7                  3                     2500   \n",
      "8                3.5                     1000   \n",
      "9                3.5                     1500   \n",
      "10               3.5                     2000   \n",
      "11               3.5                     2500   \n",
      "12                 4                     1000   \n",
      "13                 4                     1500   \n",
      "14                 4                     2000   \n",
      "15                 4                     2500   \n",
      "\n",
      "                                              params  split0_test_score  \\\n",
      "0   {'xgbc__gamma': 2.5, 'xgbc__n_estimators': 1000}          -0.276094   \n",
      "1   {'xgbc__gamma': 2.5, 'xgbc__n_estimators': 1500}          -0.265508   \n",
      "2   {'xgbc__gamma': 2.5, 'xgbc__n_estimators': 2000}          -0.259788   \n",
      "3   {'xgbc__gamma': 2.5, 'xgbc__n_estimators': 2500}          -0.255932   \n",
      "4     {'xgbc__gamma': 3, 'xgbc__n_estimators': 1000}          -0.276018   \n",
      "5     {'xgbc__gamma': 3, 'xgbc__n_estimators': 1500}          -0.265589   \n",
      "6     {'xgbc__gamma': 3, 'xgbc__n_estimators': 2000}          -0.259931   \n",
      "7     {'xgbc__gamma': 3, 'xgbc__n_estimators': 2500}          -0.256104   \n",
      "8   {'xgbc__gamma': 3.5, 'xgbc__n_estimators': 1000}          -0.276174   \n",
      "9   {'xgbc__gamma': 3.5, 'xgbc__n_estimators': 1500}          -0.265715   \n",
      "10  {'xgbc__gamma': 3.5, 'xgbc__n_estimators': 2000}          -0.260221   \n",
      "11  {'xgbc__gamma': 3.5, 'xgbc__n_estimators': 2500}          -0.256503   \n",
      "12    {'xgbc__gamma': 4, 'xgbc__n_estimators': 1000}          -0.276030   \n",
      "13    {'xgbc__gamma': 4, 'xgbc__n_estimators': 1500}          -0.265699   \n",
      "14    {'xgbc__gamma': 4, 'xgbc__n_estimators': 2000}          -0.260257   \n",
      "15    {'xgbc__gamma': 4, 'xgbc__n_estimators': 2500}          -0.256890   \n",
      "\n",
      "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
      "0           -0.275292          -0.270058        -0.273815        0.002676   \n",
      "1           -0.265608          -0.260488        -0.263868        0.002390   \n",
      "2           -0.260338          -0.254527        -0.258218        0.002619   \n",
      "3           -0.256174          -0.250220        -0.254109        0.002751   \n",
      "4           -0.275645          -0.270248        -0.273970        0.002637   \n",
      "5           -0.265801          -0.260663        -0.264018        0.002374   \n",
      "6           -0.260454          -0.254864        -0.258416        0.002521   \n",
      "7           -0.256360          -0.250541        -0.254335        0.002685   \n",
      "8           -0.275662          -0.270396        -0.274077        0.002612   \n",
      "9           -0.265864          -0.260879        -0.264153        0.002316   \n",
      "10          -0.260566          -0.255122        -0.258636        0.002489   \n",
      "11          -0.256651          -0.250899        -0.254684        0.002677   \n",
      "12          -0.275412          -0.270496        -0.273980        0.002476   \n",
      "13          -0.266003          -0.261012        -0.264238        0.002285   \n",
      "14          -0.260919          -0.255249        -0.258808        0.002531   \n",
      "15          -0.257266          -0.251481        -0.255212        0.002643   \n",
      "\n",
      "    rank_test_score  \n",
      "0                13  \n",
      "1                 9  \n",
      "2                 5  \n",
      "3                 1  \n",
      "4                14  \n",
      "5                10  \n",
      "6                 6  \n",
      "7                 2  \n",
      "8                16  \n",
      "9                11  \n",
      "10                7  \n",
      "11                3  \n",
      "12               15  \n",
      "13               12  \n",
      "14                8  \n",
      "15                4  \n"
     ]
    }
   ],
   "source": [
    "# Cross-Validation for classification\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=1126)\n",
    "\n",
    "# Selecting parameters when Grid Searching\n",
    "param_grid = {\n",
    "    'xgbc__n_estimators': [1000, 1500, 2000, 2500],\n",
    "    'xgbc__gamma': [2.5, 3, 3.5, 4]\n",
    "    #'xgbc__max_depth': [6, 7, 8],\n",
    "    #'xgbc__min_child_weight': [1, 2]\n",
    "}\n",
    "\n",
    "raw_isc_train = raw_train_df[isc_featureCols]\n",
    "gs = GridSearchCV(xgbc_pipe, param_grid, cv=kf, scoring=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n",
    "gs.fit(raw_isc_train, raw_train_df['is_canceled'])\n",
    "\n",
    "print(gs.best_params_)\n",
    "print(gs.best_score_)\n",
    "\n",
    "# Getting all the grid search results in a Pandas DataFrame\n",
    "print(pd.DataFrame(gs.cv_results_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_canceled_proba(isc_model, X_df):\n",
    "    predicted_prob = isc_model.predict_proba(X_df)\n",
    "    will_be_canceled = np.empty(shape=predicted_prob.shape[0])\n",
    "    for i in range(will_be_canceled.shape[0]):\n",
    "        will_be_canceled[i] = predicted_prob[i][1]\n",
    "    return will_be_canceled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction\n",
    "def predict_daily_rank(isc_model, adr_model, X_df):\n",
    "    # Predict \"is_canceled\"\n",
    "    will_be_canceled = predict_canceled_proba(isc_model, X_df)\n",
    "    \n",
    "    # Predict \"adr\"\n",
    "    predicted_adr = adr_model.predict(X_df)\n",
    "       \n",
    "    # Predict revenue\n",
    "    predicted_revenue = predicted_adr * (1.0 - will_be_canceled) * (np.array(X_df['stays']))\n",
    "    X_df['predicted_revenue'] = pd.Series(predicted_revenue)\n",
    "    \n",
    "    # Aggregate by date\n",
    "    daily_df = X_df.groupby(['arrival_date']).agg({'predicted_revenue':'sum'})\n",
    "    \n",
    "    thresholds = [0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n",
    "    ranks = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "    daily_df['label'] = pd.cut(daily_df.predicted_revenue, bins=thresholds,labels=ranks)\n",
    "    daily_df.reset_index(drop=False, inplace=True)\n",
    "    \n",
    "    return daily_df[['arrival_date', 'label']]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgbc_pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-db9e93071554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Make prediction on validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdaily_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_daily_rank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbc_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackreg_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'xgbc_pipe' is not defined"
     ]
    }
   ],
   "source": [
    "# Make prediction on validation data\n",
    "daily_df = predict_daily_rank(xgbc_pipe, stackreg_pipe, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = daily_df.set_index('arrival_date').join(raw_train_label_df.set_index('arrival_date'), lsuffix=\"_predicted\", rsuffix=\"_true\", how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['err'] = result_df.apply(lambda row: abs(row.label_predicted - row.label_true), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21875\n"
     ]
    }
   ],
   "source": [
    "total_error = result_df['err'].sum(axis = 0, skipna = True) / result_df.shape[0]\n",
    "print(total_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: err, dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "print(result_df['err'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-07-01</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-07-06</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-07-13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-07-14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>2017-03-11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>2017-03-13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>2017-03-15</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>2017-03-16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>192 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    arrival_date label\n",
       "0     2015-07-01     1\n",
       "1     2015-07-06     1\n",
       "2     2015-07-08     1\n",
       "3     2015-07-13     2\n",
       "4     2015-07-14     1\n",
       "..           ...   ...\n",
       "187   2017-03-11     2\n",
       "188   2017-03-13     2\n",
       "189   2017-03-15     1\n",
       "190   2017-03-16     2\n",
       "191   2017-03-17     2\n",
       "\n",
       "[192 rows x 2 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing isc_transform, total=   0.7s\n",
      "[Pipeline] ............... (step 2 of 2) Processing rfc, total=19.5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('isc_transform',\n",
       "                 ColumnTransformer(transformers=[('cat',\n",
       "                                                  Pipeline(steps=[('si',\n",
       "                                                                   SimpleImputer(fill_value='MISSING',\n",
       "                                                                                 strategy='constant')),\n",
       "                                                                  ('ohe',\n",
       "                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n",
       "                                                  ['hotel',\n",
       "                                                   'arrival_date_month',\n",
       "                                                   'arrival_date_week_number',\n",
       "                                                   'arrival_date_day_of_month',\n",
       "                                                   'meal', 'country',\n",
       "                                                   'market_segment',\n",
       "                                                   'distribution_channel',\n",
       "                                                   'is_repe...\n",
       "                                                   'stays_in_weekend_nights',\n",
       "                                                   'stays_in_week_nights',\n",
       "                                                   'adults', 'children',\n",
       "                                                   'babies', 'persons',\n",
       "                                                   'previous_cancellations',\n",
       "                                                   'previous_bookings_not_canceled',\n",
       "                                                   'booking_changes',\n",
       "                                                   'days_in_waiting_list',\n",
       "                                                   'required_car_parking_spaces',\n",
       "                                                   'total_of_special_requests'])])),\n",
       "                ('rfc',\n",
       "                 RandomForestClassifier(max_features='sqrt', n_estimators=500,\n",
       "                                        random_state=1126))],\n",
       "         verbose=100)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model with all training data\n",
    "# Train is_canceled\n",
    "isc_raw_train = raw_train_df[isc_featureCols]\n",
    "rfc_pipe.fit(isc_raw_train, raw_train_df['is_canceled'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ..... (step 1 of 2) Processing adr_transform, total=   0.7s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-e2d1b02db8f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train adr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0madr_raw_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madr_featureCols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxgbreg_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madr_raw_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbreg_pipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madr_raw_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_train_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'adr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0mfeature_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m             sample_weight_eval_set=sample_weight_eval_set, eval_group=None)\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(self, X, y, group, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, eval_group, label_transform)\u001b[0m\n\u001b[1;32m    264\u001b[0m         train_dmatrix = DMatrix(data=X, label=y, weight=sample_weight,\n\u001b[1;32m    265\u001b[0m                                 \u001b[0mbase_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_margin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                                 missing=self.missing, nthread=self.n_jobs)\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtrain_dmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, enable_categorical)\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfeature_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mfeature_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             enable_categorical=enable_categorical)\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_data_backend\u001b[0;34m(data, missing, threads, feature_names, feature_types, enable_categorical)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;34m'''Dispatch data for DMatrix.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_scipy_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_scipy_csr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_scipy_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_from_scipy_csc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_from_scipy_csr\u001b[0;34m(data, missing, feature_names, feature_types)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_uint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mc_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_size_t\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mc_array\u001b[0;34m(ctype, values)\u001b[0m\n\u001b[1;32m    259\u001b[0m             and values.dtype.itemsize == ctypes.sizeof(ctype)):\n\u001b[1;32m    260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mctype\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train adr\n",
    "adr_raw_train = raw_train_df[adr_featureCols]\n",
    "xgbreg_pipe.fit(adr_raw_train, raw_train_df['adr'])\n",
    "print(xgbreg_pipe.score(adr_raw_train, raw_train_df['adr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py:440: FutureWarning: Given feature/column names or counts do not match the ones for the data given during fit. This will fail from v0.24.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Make prediction on test data\n",
    "prediction_df = predict_daily_rank(rfc_pipe, stackreg_pipe, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arrival_date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-04-03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-04-04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-04-05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>2017-08-27</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2017-08-29</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    arrival_date label\n",
       "0     2017-04-01     3\n",
       "1     2017-04-02     2\n",
       "2     2017-04-03     3\n",
       "3     2017-04-04     1\n",
       "4     2017-04-05     3\n",
       "..           ...   ...\n",
       "148   2017-08-27     5\n",
       "149   2017-08-28     7\n",
       "150   2017-08-29     3\n",
       "151   2017-08-30     3\n",
       "152   2017-08-31     4\n",
       "\n",
       "[153 rows x 2 columns]"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    48\n",
       "3    39\n",
       "2    25\n",
       "5    17\n",
       "6    15\n",
       "7     5\n",
       "1     3\n",
       "8     1\n",
       "9     0\n",
       "0     0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_df.to_csv(\"prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regression_models/xgbreg_pipe_20000_lr-0.04_td-6_ss-0.8_cs-0.6.joblib']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save regression model\n",
    "dump(xgbreg_pipe, 'regression_models/xgbreg_pipe_20000_lr-0.04_td-6_ss-0.8_cs-0.6.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['classification_models/xgbc_pipe_td-3_ss-0.6_cs-0.5.joblib']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save classification medel\n",
    "dump(gs, 'classification_models/xgbc_pipe_td-3_ss-0.6_cs-0.5.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "rfc_pipe = load('classification_models/rfc_pipe_500_sqrt.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "stackreg_pipe = load('regression_models/stack_4gbr_4xgbr.joblib')"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
